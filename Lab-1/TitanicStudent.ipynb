{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 3.5 (Experimental) with Spark 2.0", "name": "python3-spark20"}, "language_info": {"version": "3.5.2", "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Binary Classification with Spark ML\n\n### In this notebook, we will explore Binary Classification using Spark ML. We will exploit Spark ML's high-level APIs built on top of DataFrames to create and tune machine learning pipelines. Spark ML Pipelines enable combining multiple algorithms into a single pipeline or workflow. We will heavily utilize Spark ML's feature transformers to convert, modify and scale the features that will be used to develop the machine learning model. Finally, we will evaluate and cross validate our model to demonstrate the process of determining a best fit model.\n\n### In statistics, logistic regression, or logit regression, or logit model[1] is a regression model where the dependent variable (DV) is categorical. This article covers the case of a binary dependent variable\u2014that is, where it can take only two values, \"0\" and \"1\", which represent outcomes such as pass/fail, win/lose, alive/dead or healthy/sick. Cases where the dependent variable has more than two outcome categories may be analysed in multinomial logistic regression, or, if the multiple categories are ordered, in ordinal logistic regression.[2] In the terminology of economics, logistic regression is an example of a qualitative response/discrete choice model.\n\n### The binary classification demo will utilize the famous Titanic dataset, which has been used for Kaggle competitions and can be downloaded here. There is no need to download the data manually as it is downloaded directly within the noteboook.   The Kaggle dataset (training data only) can be found here:\nhttps://www.kaggle.com/c/titanic/data\n\n\n### The Titanic data set was chosen for this binary classification demonstration because it contains both text based and numeric features that are both continuous and categorical. This will give us the opportunity to explore and utilize a number of feature transformers available in Spark ML.\n     \n          \n               \n               \n    \n\n\n![IBM Logo](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSzlUYaJ9xykGC-N5PijcV_eDBGCXy_pMn7sy6ymrVypmJ22q5ZmA)\n\n## Table of contents\n\n1. [Install needed libraries](#libraries)<br/>\n2. [Get the Data](#getdata)<br/>\n3. [Prepare and clean the data](#prepare)<br/>\n    3.1 [Remove unneeded columns](#remove)<br/>\n    3.2 [Drop rows with invalid numeric values](#drop1)<br/>\n    3.3 [Drop rows with invalid object values](#drop2)<br/>\n4. [Split the data into train and test sets](#split)<br/>\n5. [Examine the data](#examine)<br/>\n    5.1 [Sibling/Spouse](#sibsp)<br/>\n    5.2 [Parents/Children](#parch)<br/>\n    5.3 [Age](#age)<br/>\n    5.4 [Fare](#fare)<br/>\n    5.5 [Gender](#gender)<br/>\n    5.6 [Class](#pclass)<br/>\n    5.7 [Embarkation](#embarkation)<br/>\n6. [Transform the data](#transform)<br/>\n    6.1 [Gender and Embarkation](#stringindexer)<br/>\n    6.2 [Age and Fare](#bucketizer)<br/>\n7. [Build the Model](#build)<br/>\n8. [Test the Model](#test)<br/>\n9. [Tune the Model](#tune)<br/>\n10. [Predict imaginary passenger](#predict)<br/>\n11. [Random Forest](#randomforest)<br/>\n12. [Summary](#summary)<br/>"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"libraries\"></a>\n## 1. Install pixiedust and sklearn libraries (upgrade if needed)\n\n<a href=\"https://www.ibm.com/analytics/us/en/watson-data-platform/pixiedust/\">PixieDust</a> is an open source add-on created by IBM for Jupyter Notebooks to make working with data simple.<br>\n<a href=\"http://scikit-learn.org/stable/\">sklearn</a> is an open-source machine-learning library for Python."}, {"execution_count": null, "cell_type": "code", "source": "!pip install --user --upgrade pixiedust\n!pip install --user --upgrade sklearn", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Verify Spark version and existence of Spark"}, {"execution_count": null, "cell_type": "code", "source": "print('The spark version is {}.'.format(spark.version))", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Import required Spark libraries"}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import Bucketizer\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import Normalizer\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom sklearn.model_selection import train_test_split\nimport pixiedust\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\nsns.set(style='white', context='notebook', palette='deep')", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"getdata\"></a>\n## 2. Download the data and examine.\n\n-  pclass - Passenger class \n-  survived - Whether passenger survived or not (1=survived)\n-  name - Passenger name\n-  sex - Passenger sex \n-  age - Passenger age\n-  sibsp - Number of passenger siblings/spouses\n-  parch - Number of passenger parents/children\n-  ticket - ticket number\n-  fare - fare price\n-  cabin - cabin\n-  embarked - embarcation location\n-  boat - lifeboat (if used)\n-  body - body tag\n\n## Read data in as a pandas dataFrame\n### Source data is in CSV format and includes a header. We will use Pandas to infer the schema/data types."}, {"execution_count": null, "cell_type": "code", "source": "url = \"https://raw.githubusercontent.com/jpatter/ML-POT/master/titanic.csv\"\nLoadTitanicData = pd.read_csv(url)", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"prepare\"></a>\n## 3. Prepare and shape the data\n\nPixieDust is an open-source IBM library which can be used to easily and flexibly display data.\n\nUse PixieDust to examine the schema (click on the Schema line).   Try differing displays of the data using PixieDust.\n\nFor example, try showing a histogram of fare or age or pclass.    Change the renderer and see what happens.\n\n<br>\n <div class=\"panel-group\" id=\"accordion-1\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-1\" href=\"#collapse1-1\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-1\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Select the Chart icon and select Histogram</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-1\" href=\"#collapse1-2\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-2\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Select the Options button.   Drag the age (or fare or class) field to the values column.   Change number of rows to display to more than the number of rows read in (1400 will do)</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-1\" href=\"#collapse1-3\">\n        Hint 3</a>\n      </h4>\n    </div>\n    <div id=\"collapse1-3\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Change the renderer (dropdown on upper right) to seaborn</div>\n    </div>\n  </div>\n</div> "}, {"execution_count": null, "cell_type": "code", "source": "display(LoadTitanicData)", "metadata": {"pixiedust": {"displayParams": {"handlerId": "histogram", "rowCount": "1400", "rendererId": "seaborn", "valueFields": "fare,age"}}}, "outputs": []}, {"metadata": {"pixiedust": {"displayParams": {"keyFields": "survived", "rowCount": "2000", "handlerId": "lineChart", "legend": "true", "valueFields": "age", "rendererId": "seaborn", "clusterby": "sex", "kde": "true", "mpld3": "true", "rug": "true", "lineChartType": "subplots", "aggregation": "SUM"}}}, "cell_type": "markdown", "source": "<a id=\"remove\"></a>\n## 3.1 Remove unneeded columns\n\nWe are certain we can't make use of the \"boat\", \"body\" and \"home.dest\" columns, so let's remove them.   \n\nConfirm that those columns have been removed by examining the schema and data in PixieDust."}, {"execution_count": null, "cell_type": "code", "source": "TitanicData = LoadTitanicData.drop(\"boat\",1).drop(\"body\",1).drop(\"home.dest\",1)\ndisplay(TitanicData)", "metadata": {"pixiedust": {"displayParams": {"handlerId": "dataframe"}}}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Get a count of the total number of passengers and percentage of passenger survivors.\n\nNote: our count here is only for passengers.   There were a large number of crew as well."}, {"execution_count": null, "cell_type": "code", "source": "print('The total number of passengers is {}.'.format(len(TitanicData)))\nprint('The percentage of passengers who survived is {}.'.format(TitanicData['survived'].mean() * 100.0))", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Describe the data\n\nUse the <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html\">describe()</a> method to examine the data.   \n\nWhy aren't all the data fields represented?   Can you change the describe command to show all values?<br/>\n    \nWhy are the percentile values for some columns showing as NaN?<br/>\n\n<br>\n <div class=\"panel-group\" id=\"accordion-2\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-2\" href=\"#collapse2-1\">\n        Optional - change describe() to show all values</a>\n      </h4>\n    </div>\n    <div id=\"collapse2-1\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">describe(include='all')</div>\n    </div>\n  </div>\n</div> "}, {"execution_count": null, "cell_type": "code", "source": "TitanicData.describe()", "metadata": {"scrolled": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"drop1\"></a>\n## 3.2 Drop rows with columns contain no or invalid values\n\nNote how we have 1309 values for most rows -- but for age (1046) and fare (1308) we are missing values.   This is why the describe() percentiles failed -- we don't have valid data for all rows.   \n\nFor this notebook, we will drop all rows which do not have valid values using <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html\">dropna()</a> but will limit it to only age and fare columns<br/>.   \n\n<br>\n <div class=\"panel-group\" id=\"accordion-3\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-3\" href=\"#collapse3-1\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-1\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">Use the how and subset parameters</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-3\" href=\"#collapse3-2\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-2\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">how=\"any\", subset=[\"age\", \"fare\"]</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-3\" href=\"#collapse3-3\">\n        Hint 3</a>\n      </h4>\n    </div>\n    <div id=\"collapse3-3\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">TitanicData = TitanicData.dropna(how=\"any\", subset=[\"age\", \"fare\"])</div>\n    </div>\n  </div>\n</div> \nWhat are some alternatives we might have employed instead where we could have kept the rows instead of dropping them?<br/>"}, {"execution_count": null, "cell_type": "code", "source": "TitanicData = TitanicData.dropna()\nTitanicData.describe()", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"drop2\"></a>\n## 3.3 Examine all columns for null or invalid values\n\nThe values presented by describe() are only the numeric values.   We need to check all values (especially those we plan to use later such as those for gender).\n\nSome values might be null, so let's replace any nulls with NaN [Not a Number] using the <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html\">fillna()</a> function."}, {"execution_count": null, "cell_type": "code", "source": "TitanicData = TitanicData.fillna(np.nan)\n\n# Check for Null values\nTitanicData.isnull().sum()", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Drop the rows with invalid embarked values\n\nAlso, the cabin column probably has too little data across all rows to be worth using.  Question: can you think of a way to leverage cabin information?<br/>"}, {"execution_count": null, "cell_type": "code", "source": "del TitanicData['cabin']\nCleanedData = TitanicData.dropna(how=\"any\", subset=['embarked'])\nCleanedData.isnull().sum()", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"split\"></a>\n## 4. Split the data into training (80%) and testing (20%) sets using <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">test_train_split()</a>\n\nSet random_state to 1 in order to make certain this is repeatable.\nSet shuffle to True in order to randomize the data first (did you notice that the data was ordered by pclass?)\n\n<br>\n <div class=\"panel-group\" id=\"accordion-4\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-4\" href=\"#collapse4-1\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse4-1\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">test_size, random_state and shuffle</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-4\" href=\"#collapse4-2\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse4-2\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">test_size should be set to the desired percentage (0.2)<br/>random_state should be set to a fixed value (i.e. 1)<br/>shuffle should be set to True</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-4\" href=\"#collapse4-3\">\n        Hint 3</a>\n      </h4>\n    </div>\n    <div id=\"collapse4-3\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">train, test = train_test_split(CleanedData, test_size = 0.2, random_state=1, shuffle=True)</div>\n    </div>\n  </div>\n</div> "}, {"execution_count": null, "cell_type": "code", "source": "train, test = train_test_split(CleanedData, )\ntrain.head(5)", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"examine\"></a>\n## 5. Examine the data\n\nExamine the data to determine which values we may want to include in the model and whether we need to perform any additional data shaping.\n\n## Examine correlations between the numeric values in the data set to survived using a <a href=\"https://seaborn.pydata.org/generated/seaborn.heatmap.html\">heatmap</a>.\n\nWe can see the closest collelation between fare and survived (if you think about it this makes sense).   This doesn't mean that the other values are not useful. But to determine this, we need to explore in detail the remaining features.\n\nTo do this (as we did here) we will only look at the training data.   Why wouldn't we use the test data as well?<br/>\n\n<br/>\n<div class=\"panel-group\" id=\"accordion-5\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-5\" href=\"#collapse5-1\">\n        Hint 1</a>\n      </h4>\n    </div>\n    <div id=\"collapse5-1\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">\"survived\",\"pclass\"</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-5\" href=\"#collapse5-2\">\n        Hint 2</a>\n      </h4>\n    </div>\n    <div id=\"collapse5-2\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">\"survived\",\"pclass\",\"sibsp\",\"parch\"</div>\n    </div>\n  </div>\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-5\" href=\"#collapse5-3\">\n        Hint 3</a>\n      </h4>\n    </div>\n    <div id=\"collapse5-3\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">g = sns.heatmap(train[[\"survived\",\"pclass\",\"sibsp\",\"parch\",\"age\",\"fare\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")</div>\n    </div>\n  </div>\n</div> "}, {"execution_count": null, "cell_type": "code", "source": "g = sns.heatmap(train[[<insert fields here>]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## In-depth data examination -- we will review these in the class, so if you are behind, you can skip to [Transform the data](#transform).\n\n<a id=\"sibsp\"></a>\n## 5.1 Sibling/Spouse vs Survived using <a href=\"https://seaborn.pydata.org/generated/seaborn.factorplot.html\">factorplot()</a>\n\nIt seems that passengers having a lot of siblings/spouses have less chance to survive.   Perhaps they refused to break apart and chose to face their fate together.<br>\nSingle passengers (0 SibSP) or with two other persons (SibSP 1 or 2) have more chance to survive"}, {"execution_count": null, "cell_type": "code", "source": "# Explore SibSp feature vs Survived\ng = sns.factorplot(x=\"sibsp\",y=\"survived\",data=train,kind=\"bar\",size = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"parch\"></a>\n## 5.2 Parents/Children vs Survived\n\nSmall families have more chance to survive, more than single (Parch 0), medium (Parch 3,4) and large families (Parch 5,6 ) -- this appears to complement the findings for sibsp.<br>\nOf interest is a significant standard deviation in the survival of passengers with three (3) parents/children"}, {"execution_count": null, "cell_type": "code", "source": "# Explore Parch feature vs Survived\ng  = sns.factorplot(x=\"parch\",y=\"survived\",data=train,kind=\"bar\", size = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"age\"></a>\n## 5.3 Age vs Survived using <a href=\"https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\">FacetGrid()</a>\n\nAge distributions are not the same in the survived and not survived subpopulations. Indeed, there is a peak corresponding to young passengers, that have survived. We also see that passengers between 60-80 have less chance to survive.\nSo, even if \"age\" is not correlated with \"survived\", we can see that there is age categories of passengers that of have more or less chance to survive.   Young adults have a good chance to survive perhaps because they could survive in the water longer (this did happen).\n\nIt seems that very young passengers have more chance to survive [likely they got priority in the lifeboats]."}, {"execution_count": null, "cell_type": "code", "source": "# Explore Age vs Survived\ng = sns.FacetGrid(train, col='survived')\ng = g.map(sns.distplot, \"age\")", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## When we superimpose the two densities , we cleary see a peak correponsing (between 0 and 5) to babies and very young children."}, {"execution_count": null, "cell_type": "code", "source": "# Explore Age distibution \ng = sns.kdeplot(train[\"age\"][(train[\"survived\"] == 0) & (train[\"age\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train[\"age\"][(train[\"survived\"] == 1) & (train[\"age\"].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"fare\"></a>\n## 5.4 Fare distribution is very <a href=\"https://en.wikipedia.org/wiki/Skewness\">skewed</a>. This can lead to overweight values in the model, even if it is scaled.\n\nIn this case, it is better to transform it with a log function to reduce this skew."}, {"execution_count": null, "cell_type": "code", "source": "# Explore Fare distribution \ng = sns.distplot(train[\"fare\"], color=\"m\", label=\"Skewness : %.2f\"%(train[\"fare\"].skew()))\ng = g.legend(loc=\"best\")", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Map a log() function to all values of fare -- both train *and* test!\n\nIf we are going to modify the training data, we need to make certain the test data also reflects the same change."}, {"execution_count": null, "cell_type": "code", "source": "# Apply log to Fare to reduce skewness distribution\ntrain[\"fare\"] = train[\"fare\"].map(lambda i: np.log(i) if i > 0 else 0)\ntest[\"fare\"] = test[\"fare\"].map(lambda i: np.log(i) if i > 0 else 0)", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "g = sns.distplot(train[\"fare\"], color=\"b\", label=\"Skewness : %.2f\"%(train[\"fare\"].skew()))\ng = g.legend(loc=\"best\")", "metadata": {"collapsed": true, "scrolled": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"gender\"></a>\n## 5.5 Survival rates by Gender\n\nIt is clearly obvious that Males have less chance to survive than Females.\nSo gender likely plays an important role in the survival prediction.\n\nRemember this sentence during the evacuation : \"Women and children first\"."}, {"execution_count": null, "cell_type": "code", "source": "g = sns.barplot(x=\"sex\",y=\"survived\",data=train)\ng = g.set_ylabel(\"Survival Probability\")", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "train[[\"sex\",\"survived\"]].groupby('sex').mean()", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"pclass\"></a>\n## 5.6 Survival by Passenger Class\n\nThe passenger survival rate is not the same across all three (3) classes. First class passengers have more chance to survive than second class and third class passengers.\nThis trend extends to when we look at both male and female passengers by class."}, {"execution_count": null, "cell_type": "code", "source": "# Explore Pclass vs Survived\ng = sns.factorplot(x=\"pclass\",y=\"survived\",data=train,kind=\"bar\", size = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "# Explore Pclass vs Survived by Sex\ng = sns.factorplot(x=\"pclass\", y=\"survived\", hue=\"sex\", data=train,\n                   size=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"embarkation\"></a>\n## 5.7 Survival by Embarkation Location\n\nPassengers embarking at Cherbourg have a better chance of survival than Queenstown (Q) or Southampton (S).   Any thoughts as to why?"}, {"execution_count": null, "cell_type": "code", "source": "# Explore Embarked vs Survived \ng = sns.factorplot(x=\"embarked\", y=\"survived\",  data=train,\n                   size=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Examine Embarked by Class\n\nLooks like most first class passengers embarked in Cherbourg (C).   "}, {"execution_count": null, "cell_type": "code", "source": "# Explore Pclass vs Embarked \ng = sns.factorplot(\"pclass\", col=\"embarked\",  data=train,\n                   size=6, kind=\"count\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Count\")", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"transform\"></a>\n## 6. Transform the data\n\nCertain data fields need to be transformed before building the model.   This can be for several reasons ranging from needing to convert String values to numeric values or shaping data into different formats.\n\n<a id=\"stringindexer\"></a>\n## 6.1 Use <a href=\"https://spark.apache.org/docs/latest/ml-features.html#stringindexer\">StringIndexer</a> to transform gender and embarked values\n\nStringIndexer is a transformer that encodes a string column to a column of indices. The indices are ordered by value frequencies, so the most frequent value gets index 0. If the input column is numeric, it is cast to string first. \n\nFor the Titanic data set, we will index the Sex/Gender column as well as the Embarked column, which specifies at which  port the passenger boarded the ship."}, {"execution_count": null, "cell_type": "code", "source": "SexIndexer = StringIndexer(inputCol=\"sex\", outputCol=\"SexIndex\")\nEmbarkedIndexer = StringIndexer(inputCol=\"embarked\", outputCol=\"EmbarkedIndex\")", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"bucketizer\"></a>\n## 6.2 <a href=\"https://spark.apache.org/docs/latest/ml-features.html#bucketizer\">Bucketizer</a> is a transformer that transforms a column of continuous features to a column of feature buckets, where the buckets are by a splits parameter. \n\nFor the Titanic data set, we will index the Age and Fare features.\n\nImportant!   Note that for Fare the splits now correspond to the log values since we made that change.\n\n<br/>\n<div class=\"panel-group\" id=\"accordion-6\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-6\" href=\"#collapse6-1\">\n        Advanced Optional</a>\n      </h4>\n    </div>\n    <div id=\"collapse6-1\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">After completing the lab, note the prediction percentage then come back and change the values for either Bucketizer and re-run the kernel [Kernel->Restart and Run All].   Note the change in prediction accuracy.</div>\n    </div>\n  </div>\n</div> "}, {"execution_count": null, "cell_type": "code", "source": "AgeBucketSplits = [0.0, 6.0, 12.0, 18.0, 40.0, 65.0, 80.0, float(\"inf\")]\nAgeBucket = Bucketizer(splits=AgeBucketSplits, inputCol=\"age\", outputCol=\"AgeBucket\")\n\nFareBucketSplits = [-float(\"inf\"), 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, float(\"inf\")]\nFareBucket = Bucketizer(splits=FareBucketSplits, inputCol=\"fare\", outputCol=\"FareBucket\")", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"build\"></a>\n## 7. Building the Model\n\n## <a href=\"https://spark.apache.org/docs/latest/ml-features.html#vectorassembler\">VectorAssembler</a> is a transformer that combines a given list of columns in the order specified into a single vector column in order to train a model.\n\n<br/>\n<div class=\"panel-group\" id=\"accordion-7\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-7\" href=\"#collapse7-1\">\n        Advanced Optional</a>\n      </h4>\n    </div>\n    <div id=\"collapse7-1\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">After completing the lab, note the prediction percentage then come back and remove some of the values in the assembler (i.e. remove sibsp, pclass and parch or remove SexIndex) and re-run the kernel [Kernel->Restart and Run All].   Note the change in prediction accuracy.</div>\n    </div>\n  </div>\n</div> "}, {"execution_count": null, "cell_type": "code", "source": "assembler = VectorAssembler(inputCols= [\"SexIndex\", \"EmbarkedIndex\", \"AgeBucket\", \"FareBucket\", \"sibsp\", \"pclass\", \"parch\"], outputCol=\"features\")", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## <a href=\"https://spark.apache.org/docs/latest/ml-features.html#normalizer\">Normalizer</a> is a Transformer which transforms a dataset of Vector rows, normalizing each Vector to have unit norm\n### This normalization can help standardize your input data and improve the behavior of learning algorithms."}, {"execution_count": null, "cell_type": "code", "source": "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## <a href=\"https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression\">Logistic regression</a> is a popular method to predict a binary response (Survived/Did Not Survive)\n### It is a special case of Generalized Linear models that predicts the probability of an outcome.\n\n<br/>\n<div class=\"panel-group\" id=\"accordion-7\">\n  <div class=\"panel panel-default\">\n    <div class=\"panel-heading\">\n      <h4 class=\"panel-title\">\n        <a data-toggle=\"collapse\" data-parent=\"#accordion-7\" href=\"#collapse7-2\">\n        Advanced Optional</a>\n      </h4>\n    </div>\n    <div id=\"collapse7-2\" class=\"panel-collapse collapse\">\n      <div class=\"panel-body\">After completing the lab, note the prediction percentage then come back and change maxIter (say to 50 or 100) and re-run the kernel [Kernel->Restart and Run All].   Note the change in prediction accuracy.</div>\n    </div>\n  </div>\n</div> "}, {"execution_count": null, "cell_type": "code", "source": "lr = LogisticRegression(featuresCol=\"normFeatures\", labelCol=\"survived\", predictionCol=\"prediction\", maxIter=10, regParam=0.01, elasticNetParam=0.8)", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## A <a href=\"https://spark.apache.org/docs/latest/ml-pipeline.html\">Pipeline</a> is a sequence of stages where each stage is either a Transformer or an Estimator\n### These stages are run in order and the input DataFrame is transformed as it passes through each stage. \n\n### In machine learning, it is common to run a sequence of algorithms to process and learn from data.\n\nWe want to run the indexers and bucketizers first, then the assembler, normalizer -- and finally the logistic regression."}, {"execution_count": null, "cell_type": "code", "source": "pipeline = Pipeline(stages=[SexIndexer, EmbarkedIndexer, AgeBucket, FareBucket, assembler, normalizer, lr])", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Transform the test and training sets back from Pandas dataframes to Spark dataframes.\n### We need to do this because the ML algorithms require Spark dataframes.   They will not work with Pandas.\n### Cache the resulting DataFrames - this is simply to improve performance."}, {"execution_count": null, "cell_type": "code", "source": "train_df = spark.createDataFrame(train)\ntest_df = spark.createDataFrame(test)\n\ntrain_df.cache()\ntest_df.cache()\nprint('The number of records in the training data set is {}.'.format(train_df.count()))\nprint('The number of rows labeled Not Survived in the training data set is {}.'.format(train_df.filter(train_df['survived'] == 0).count()))\nprint('The number of rows labeled Survived in the training data set is {}.'.format(train_df.filter(train_df['survived'] == 1).count()))\ntrain_df.sample(False, 0.01, seed=0).show(5)\nprint('')\n\nprint('The number of records in the test data set is {}.'.format(test_df.count()))\nprint('The number of rows labeled Not Survived in the test data set is {}.'.format(test_df.filter(test_df['survived'] == 0).count()))\nprint('The number of rows labeled Survived in the test data set is {}.'.format(test_df.filter(test_df['survived'] == 1).count()))\ntest_df.sample(False, 0.1, seed=0).show(5)", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Fit the pipeline to the training data"}, {"execution_count": null, "cell_type": "code", "source": "model = pipeline.fit(train_df)", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"test\"></a>\n## 8. Make predictions on passengers in the Test data set\n### Keep in mind that the model has not seen the data in the test data set"}, {"execution_count": null, "cell_type": "code", "source": "predictions = model.transform(test_df)", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Obtain a sample of the predictions and view using PixieDust\n\nYou can see the survived value on the far left side of the table and the prediction on the far right."}, {"execution_count": null, "cell_type": "code", "source": "display(predictions.sample(False, 0.1, seed=0))", "metadata": {"collapsed": true, "pixiedust": {"displayParams": {"handlerId": "dataframe"}}}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "print('The number of predictions labeled Not Survived is {}.'.format(predictions.filter(predictions['prediction'] == 0).count()))\nprint('The number of predictions labeled Survived is {}.'.format(predictions.filter(predictions['prediction'] == 1).count()))", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "(predictions.filter(\"Survived = 0.0\")\n     .select(\"Sex\", \"Age\", \"Fare\", \"Embarked\", \"Pclass\", \"Parch\", \"SibSp\", \"Survived\", \"prediction\")\n     .sample(False, 0.1, seed=0).show(5))\n\n(predictions.filter(\"Survived = 1.0\")\n     .select(\"Sex\", \"Age\", \"Fare\", \"Embarked\", \"Pclass\", \"Parch\", \"SibSp\", \"Survived\", \"prediction\")\n     .sample(False, 0.5, seed=0).show(5))", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "## Create an evaluator for the <a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/ml/evaluation/BinaryClassificationEvaluator.html\">binary classification</a> using area under the ROC Curve as the evaluation metric\n\n### Receiver operating characteristic (ROC) is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied\n\nThe curve is created by plotting the true positive rate against the false positive rate at various threshold settings. The ROC curve is thus the sensitivity as a function of fall-out. The area under the ROC curve is useful for comparing and selecting the best machine learning model for a given data set. A model with an area under the ROC curve score near 1 has very good performance. A model with a score near 0.5 is about as good as flipping a coin."}, {"execution_count": null, "cell_type": "code", "source": "evaluator = BinaryClassificationEvaluator().setLabelCol(\"survived\").setMetricName(\"areaUnderROC\")\nprint('Area under the ROC curve = {}.'.format(evaluator.evaluate(predictions)))", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"tune\"></a>\n## 9. Tune the Model\n\n## Tune Hyperparameters\n### Generate hyperparameter combinations by taking the cross product of some parameter values\n\nSpark ML algorithms provide many hyperparameters for tuning models. These hyperparameters are distinct from the model parameters being optimized by Spark ML itself. Hyperparameter tuning is accomplished by choosing the best set of parameters based on model performance on test data that the model was not trained with. All combinations of hyperparameters specified will be tried in order to find the one that leads to the model with the best evaluation result."}, {"metadata": {}, "cell_type": "markdown", "source": "## Build a <a href=\"https://spark.apache.org/docs/latest/ml-tuning.html\">Parameter Grid</a> specifying what parameters and values will be evaluated in order to determine the best combination"}, {"execution_count": null, "cell_type": "code", "source": "paramGrid = (ParamGridBuilder().addGrid(lr.regParam, [0.0, 0.1, 0.3])\n                 .addGrid(lr.elasticNetParam, [0.0, 1.0])\n                 .addGrid(normalizer.p, [1.0, 2.0])\n                 .build())", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Create a <a href=\"https://spark.apache.org/docs/latest/ml-tuning.html\">cross validator</a> to tune the pipeline with the generated parameter grid\nSpark ML provides for cross-validation for hyperparameter tuning. Cross-validation attempts to fit the underlying estimator with user-specified combinations of parameters, cross-evaluate the fitted models, and output the best one."}, {"execution_count": null, "cell_type": "code", "source": "cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(10)", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Cross-evaluate the ML Pipeline to find the best model\n### using the area under the ROC evaluator and hyperparameters specified in the parameter grid"}, {"execution_count": null, "cell_type": "code", "source": "# CleanedData did not contain the fare transformation so we need to add it here.\nCleanedData[\"fare\"] = CleanedData[\"fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n\nTitanicData_df = spark.createDataFrame(CleanedData)\ncvModel = cv.fit(TitanicData_df)\nprint('Area under the ROC curve for best fitted model = {}.'.format(evaluator.evaluate(cvModel.transform(TitanicData_df))))", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Let's see what improvement we achieve by tuning the hyperparameters using cross-evaluation "}, {"execution_count": null, "cell_type": "code", "source": "print('Area under the ROC curve for non-tuned model = {}.'.format(evaluator.evaluate(predictions)))\nprint('Area under the ROC curve for best fitted model = {}.'.format(evaluator.evaluate(cvModel.transform(TitanicData_df))))\nprint('Improvement = {0:0.2f}%'.format((evaluator.evaluate(cvModel.transform(TitanicData_df)) - evaluator.evaluate(predictions)) *100 / evaluator.evaluate(predictions)))", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Make improved predictions using the Cross-validated model\n### Using the Test data set and DataFrame API"}, {"execution_count": null, "cell_type": "code", "source": "cvModel.transform(test_df).select(\"survived\", \"prediction\").sample(False, 0.1, seed=0).show(10)", "metadata": {"collapsed": true, "scrolled": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Like above, but now using SQL"}, {"execution_count": null, "cell_type": "code", "source": "# create temporary table\ncvModel.transform(test_df).createOrReplaceTempView(\"cvModelPredictions\")\nspark.sql(\"select survived, prediction from cvModelPredictions\").sample(False, 0.1, seed=0).show(10)", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"predict\"></a>\n## 10. Make a prediction on an imaginary passenger"}, {"metadata": {}, "cell_type": "markdown", "source": "## Define the imaginary passenger's features"}, {"execution_count": null, "cell_type": "code", "source": "SexValue = 'female'\nAgeValue = 40.0\n\n# remember fare values were transformed to the log -- so values should be -1 to 8\nFareValue = 8.0\n\nEmbarkedValue = 'C'\nPclassValue = 2\nSibSpValue = 1\nParchValue = 1\n\nPredictionFeatures = (spark.createDataFrame([(SexValue, AgeValue, FareValue, EmbarkedValue, PclassValue, SibSpValue, ParchValue)],\n    ['sex', 'age', 'fare', 'embarked', 'pclass', 'sibsp', 'parch']))\nPredictionFeatures.show()", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Predict whether the imaginary person would have survived\n### using the best fit model"}, {"execution_count": null, "cell_type": "code", "source": "SurvivedOrNotPrediction = cvModel.transform(PredictionFeatures)\nSurvivedOrNotPrediction.select('rawPrediction', 'probability', 'prediction').show(1, False)", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Display Prediction Result"}, {"execution_count": null, "cell_type": "code", "source": "SurvivedOrNot = SurvivedOrNotPrediction.select(\"prediction\").first()[0]\nif SurvivedOrNot == 0.0:\n    print(\"Did NOT Survive\")\nelif(SurvivedOrNot == 1.0):\n    print(\"Did Survive!!!\")\nelse:\n    print(\"Invalid Prediction\")", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"randomforest\"></a>\n## 11. Let's take a quick look at applying the feature engineering performed above to a Random Forest Model\n### Random forests are ensembles of decision trees. They combine many decision trees in order to reduce the risk of overfitting.\n### We won't do any hyperparamter tuning in this example, but just show how to create and evaluate the model using all default hyperparameters"}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.ml.classification import RandomForestClassificationModel, RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.feature import IndexToString\n\n# Index labels, adding metadata to the label column.\n# Fit on whole dataset to include all labels in index.\nlabelIndexer = StringIndexer().setInputCol(\"survived\").setOutputCol(\"indexedLabel\").fit(TitanicData_df)\n\n# Train a RandomForest model\nrf = RandomForestClassifier().setLabelCol(\"indexedLabel\").setFeaturesCol(\"features\").setNumTrees(20)\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(labelIndexer.labels)\n\n# Create new Pipeline using the RandomForest model and all the same feature transformers used above for logistic regression\npipelineRF = Pipeline().setStages([labelIndexer, SexIndexer, EmbarkedIndexer, AgeBucket, FareBucket, assembler, normalizer, rf, labelConverter])\n\n# Train model.\nmodelRF = pipelineRF.fit(train_df)\n\n# Make predictions.\npredictionsRF = modelRF.transform(test_df)\n\n# Select example rows to display.\npredictionsRF.select(\"predictedLabel\", \"survived\", \"features\").show(10)\n\n# Select (prediction, true label) and compute test error\nevaluatorRF = MulticlassClassificationEvaluator().setLabelCol(\"survived\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\naccuracyRF = evaluatorRF.evaluate(predictionsRF)\nprint(\"Test Error = %g\" % (1.0 - accuracyRF))\n\nrfModel = modelRF.stages[7]\nprint(rfModel)  # summary only", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"summary\"></a>\n## 12. Summary and next steps\n\nYou created a predictive model that predicts survival probabilities for passengers on the Titanic.\n\n  - Load the data\n  - Cleaned the data\n  - Split the data into training and test sets\n  - Examined data in the training set to determine which data to use and how it needed to be shaped\n  - Created transformers to shape the data\n  - Created a model using Pipeline\n  - Tested the model\n  - Tuned the model\n  - Tested the model on an imaginary passenger\n  - Build a second model using Random Forest\n  \n\n### Authors\n\nJoel Patterson - IBM Corporation\n\nBased in part on work by Rich Tarro (IBM) and Yassine Ghouzam, PhD"}, {"metadata": {}, "cell_type": "markdown", "source": "![IBM Logo](http://www-03.ibm.com/press/img/Large_IBM_Logo_TN.jpg)"}, {"execution_count": null, "cell_type": "code", "source": "", "metadata": {"collapsed": true}, "outputs": []}], "nbformat": 4, "nbformat_minor": 1}
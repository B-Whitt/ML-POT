{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from Keras dataset<br>\n",
    "Returns two tuples:<br>\n",
    "X, X_test: uint8 array of RGB image data with shape (num_samples, 3, 32, 32).<br>\n",
    "y, y_test: uint8 array of category labels (integers in range 0-9) with shape (num_samples,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "(X,y), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training set into training and validation sets:\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data to project filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cifar-10-tf-train.pkl', 'wb') as f:\n",
    "    pickle.dump((X_train, y_train), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cifar-10-tf-valid.pkl', 'wb') as f:\n",
    "    pickle.dump((X_valid, y_valid), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cifar-10-tf-test.pkl', 'wb') as f:\n",
    "    pickle.dump((X_test, y_test), f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibm_boto3\n",
    "from ibm_botocore.client import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Cloud Object Storage credentials \n",
    "\n",
    "1. Select the Data icon (upper right, icon with 1 and 0s)\n",
    "2. Select Connections\n",
    "3. Select the cell below and then select <b>Insert to code</b> for <b>Connection to project COS</b> to insert credentials\n",
    "4. Rename the credentials (which are usually called credentials_1) to cos_credentials.   YOU WILL GET ERRORS IF YOU DO NOT DO THIS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_endpoint = cos_credentials['url']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_endpoint = cos_credentials['iam_url']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = ibm_boto3.client('s3',\n",
    "                      ibm_api_key_id=cos_credentials['api_key'],\n",
    "                      ibm_service_instance_id=cos_credentials['resource_instance_id'],\n",
    "                      ibm_auth_endpoint=auth_endpoint,\n",
    "                      config=Config(signature_version='oauth'),\n",
    "                      endpoint_url=service_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bucket name -- I usually append my name to the end (i.e. cifar10-tutorials-joel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"cifar10-tutorials-test-blb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This call may fail if bucket has already been created -- that's fine, simply continue executing the next cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos.create_bucket(Bucket=bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['cifar-10-tf-train.pkl',\n",
    "         'cifar-10-tf-valid.pkl',\n",
    "         'cifar-10-tf-test.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    print('Uploading data {}...', format(file))\n",
    "    cos.upload_file(Filename=file, Bucket=bucket, Key=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos.list_objects(Bucket=bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The number of the image we wish to test from the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number = 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cifar-10-tf-valid.pkl\", \"rb\") as f:\n",
    "  cifar_test_data=pickle.load(f)\n",
    "cifar_test_data[0][image_number].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (cifar_test_data[1][110])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{"metadata": {"kernelspec": {"name": "python3-spark20", "display_name": "Python 3.5 (Experimental) with Spark 2.0", "language": "python"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "version": "3.5.2", "file_extension": ".py", "nbconvert_exporter": "python"}}, "cells": [{"metadata": {}, "cell_type": "markdown", "source": "## <span style=\"color:#fa04d9\"><center>PRINCIPAL COMPONENT ANALYSIS USING SPARK</center></span>\n\n### Suppose we have the following simple dataset composed of 5 datapoints with (x,y) coordinates: A(-4, -1), B(-2 0), C(0, 1), D(2, 2) and E(4, 3)\n\n![figure1](https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/pictures/figure1.jpg)\n\n### It is very straightforward to notice that these 5 data points are forming a straight line (with slope equal to 0.5) and could therefore be described with a single dimension vector such as the vector V(2,1) or any multiple of it, as described in the picture below.<br><br>\n![figure2](https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/pictures/figure2.jpg)\n\n### The figure above represents the straight line (in light blue) passing through points A, B, C, D and E and two example vectors, V of coordinates (2,1) (in red) and U of coordinates (-3, -1.5) (in green) which can serve as a base for a single dimensional set of coordinates for the points on this line.\n\n### Let us now suppose that the fact that our data set can be described by a single dimension was not obvious, (this is typically the case with more complex scenarios, where the dataset can be described fairly well with less dimensions than initially given) and work on \"discovering\" this property. This can be done using the mathematical method known as \"Principal Component Analysis\" which is implemented in Apache Spark as \"PCA\"."}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# <span style=\"color:#fa04d9\">**Step 1: Define the Spark context variable**</span>"}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()", "execution_count": 1}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# <span style=\"color:#fa04d9\"> Step 2: Create the dataset corresponding to the five data point described above as a dataframe.</span>"}, {"metadata": {}, "cell_type": "markdown", "source": "Virtually all Spark machine learning implementations take Vectors of features as input (rather than individual columns), so we will build our data frame as a set of 5 rows, where each row represents one of the data points from above. "}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "from pyspark.ml.linalg import Vectors\ndata = [(Vectors.dense([0.0, 1.0]),), \n        (Vectors.dense([2.0, 2.0]),),\n        (Vectors.dense([4.0, 3.0]),),\n        (Vectors.dense([-2.0, 0.0]),),\n        (Vectors.dense([-4.0, -1.0]),)\n        ]\n\ndf = spark.createDataFrame(data, [\"features\"])", "execution_count": 2}, {"metadata": {}, "cell_type": "markdown", "source": "### Verify the correctness of the data frame by displaying it."}, {"metadata": {}, "cell_type": "code", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------+\n|   features|\n+-----------+\n|  [0.0,1.0]|\n|  [2.0,2.0]|\n|  [4.0,3.0]|\n| [-2.0,0.0]|\n|[-4.0,-1.0]|\n+-----------+\n\n"}], "source": "df.show()", "execution_count": 3}, {"metadata": {}, "cell_type": "markdown", "source": "### <span style=\"color:green\">Remark: If you are wondering about the syntax used above to create the data frame, where each Vector seems to be \"wrapped\" into an extra set of brackets with an extra comma, you will find below a quick clarification<span>"}, {"metadata": {}, "cell_type": "markdown", "source": "<span style=\"color:green\">The reason for this is that SparkSession.createDataFrame(), which is used under the hood, requires an RDD / list of Row/tuple/list/ or pandas.DataFrame, unless a schema with DataType is provided. This is better explained with an example.<br> Consider trying to do the following to create a data frame in Python:<br><br>data = ['a', 'b', 'c']<br>df = spark.createDataFrame(data, [\"features\"])<br><br>\nThis code above will fail with a message such as: <span style=\"color:blue\">TypeError: schema should be StructType or list or None, but got: set(['features'])</span> <br><br>\nYou can then consider a couple of ways to fix this.\n\n### <span style=\"color:green\">Method 1: Declare the schema type using a StructType directly in the invocation of createDataFrame.\n<span style=\"color:green\">from pyspark.sql.types import StringType<br> \ndf = spark.createDataFrame(data, StringType(), [\"features\"])\n\n### <span style=\"color:green\">Method 2: You can alternatively feed tuples into createDataFrame (rather than single values). This is where the Python syntax to create single element tuples is used. In <span style=\"color:green\">Python, (a,) represents a one-element tuple containing just a.\n<span style=\"color:green\">data = [('a',), ('b',), ('c',)]<br>\ndf = spark.createDataFrame(data, [\"features\"])<br><br>This code should now be successful.\n### <span style=\"color:green\">Closing the remark and resuming with the PCA tutorial."}, {"metadata": {}, "cell_type": "markdown", "source": "# <span style=\"color:#fa04d9\">Step 3: Instantiate and train the PCA model in Spark. </span>"}, {"metadata": {}, "cell_type": "markdown", "source": "First, we will instantiate a standard scaler. The original data will be transformed to have a mean of 0, which is a very common data preparation technic in data science. This implies in our case that our set of 5 data points will be centered around the origin so that the sum of all X values and the sum of all Y values add up to 0."}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "from pyspark.ml.feature import StandardScaler\nscaler_definition = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n                        withStd=False, withMean=True)", "execution_count": 4}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "# As implied by the names used for the variables, scaler_definition is only a definition of the standard scaler. In order to obtain an actual instance, we need to apply the \"fit\" method to the definition,\n# passing in the actual data.\nscaler_instance_trained = scaler_definition.fit(df)", "execution_count": 5}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "# The trained instance of the scaler can now be used with the \"transform\" method, to take in the original dataset (a dataframe of feature vectors) and produce as output a \n# new dataframe where the data now has a 0 mean.\nscaler_output_df = scaler_instance_trained.transform(df)", "execution_count": 6}, {"metadata": {}, "cell_type": "code", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------+--------------+\n|   features|scaledFeatures|\n+-----------+--------------+\n|  [0.0,1.0]|     [0.0,0.0]|\n|  [2.0,2.0]|     [2.0,1.0]|\n|  [4.0,3.0]|     [4.0,2.0]|\n| [-2.0,0.0]|   [-2.0,-1.0]|\n|[-4.0,-1.0]|   [-4.0,-2.0]|\n+-----------+--------------+\n\n"}], "source": "# Verify that the newly produced data is indeed scaled to have a 0 mean.\nscaler_output_df.show()", "execution_count": 7}, {"metadata": {}, "cell_type": "markdown", "source": "Now that we have our scaledFeatures, we can create our PCA model which will be expecting an input column named \"scaledFeatures\" and will produce as output a column (of vectors) named \"pcaFeatures\". This notebook will detail the meaning of each."}, {"metadata": {}, "cell_type": "markdown", "source": "## Important Remark:<br>\nThe meaning of the parameter k=2 (in the cell below) indicates that we want our PCA model to produce a description of our data in TWO dimensions. This may seem counter intuitive as we 'already' know that our dataset is a straight line and therefore unidimensional. Remember that we are working with a very simple example where we are pretending not to be aware of the end result. In general, when starting with an N dimensional dataset, it is common to use PCA with a value of N since it is not initially known what the \"meaningful\" number of final dimensions is. As a matter of fact, as we will see shortly, it is one of the outputs of PCA, once it produces a new set of dimensions describing the dataset, to also provide the amount of 'variance' captured by each one of those dimensions. <span style=\"color:red\">**It is then up to the data scientist to decide how many dimensions to keep in order to get the best *bang for the buck* **.</span>"}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "from pyspark.ml.feature import PCA\npca_model_definition = PCA(k=2, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")", "execution_count": 8}, {"metadata": {}, "cell_type": "markdown", "source": "Using the same approach as with the standard scaler a few cells above, we create a PCA class object (think of this as a class capable of producing an actual Model, once it has been given a dataset to work with and invoked the 'fit' method on that dataset) and then subsequently make that definition into a *real* instantiated model by fitting it to the output of the standard scaler produced higher."}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "pca_model_instance = pca_model_definition.fit(scaler_output_df)", "execution_count": 9}, {"metadata": {}, "cell_type": "markdown", "source": "The variable \"pca_model_instance\" is the actual PCA model which was obtained using our simple 5 points dataset. **The Spark implementation class type of this object is <span style=\"color:red\">\"PCAModel\"**</span>\n\nThe Spark documentation provides a list of methods which can be invoked on this pca_model_instance that we just obtained. One method of interest is \"pc\" and stands for \"principal components\". This method returns a matrix which we will look at more closely after displaying it first."}, {"metadata": {}, "cell_type": "code", "outputs": [{"metadata": {}, "output_type": "execute_result", "execution_count": 10, "data": {"text/plain": "DenseMatrix(2, 2, [-0.8944, -0.4472, -0.4472, 0.8944], 0)"}}], "source": "pca_model_instance.pc", "execution_count": 10}, {"metadata": {}, "cell_type": "markdown", "source": "We can see that this matrix has two column/vectors (represented by the first '2' in the output above) and that each column/vector has two coordinates (represented by the second '2'). We can also print this matrix with a slightly better formatting as in the cell below."}, {"metadata": {}, "cell_type": "code", "outputs": [{"output_type": "stream", "name": "stdout", "text": "DenseMatrix([[-0.89442719, -0.4472136 ],\n             [-0.4472136 ,  0.89442719]])\n"}], "source": "print (pca_model_instance.pc)", "execution_count": 11}, {"metadata": {}, "cell_type": "markdown", "source": "# <span style=\"color:#fa04d9\">Step 4: Interpreting the output of the PCA implementation in Spark.</span>"}, {"metadata": {}, "cell_type": "markdown", "source": "## &nbsp;&nbsp;&nbsp;<span style=\"color:#fa04d9\">Step 4.1: Output of the method \"pc\" from the Spark PCAModel class: The Eigen Vectors. </span>\nWe can represent this matrix as two vectors W and X (or PC1 and PC2, for Principal Component 1 and Principal Component 2 returned by the Spark PCA algorithm) with the corresponding coordinates as can be seen below (coordinates rounded to two decimal significant digits).\n\n![eigen vectors](https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/pictures/eigen_vectors.jpg)\n\nThe figure below also provides a visual representation of these two dimensions (W and X) returned by PCA, in the context of the original dataset with the five data points A, B, C, D and E and the original vector V(2,1), all represented at a larger scale than the original picture at the beginnig of this notebook. (Note that vector U has been left out to avoid overloading the diagram).\n![figure3](https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/pictures/figure3.jpg)\n\n### <span style=\"color:blue\">Some important remarks about the results obtained so far:</span><br>\n1- The dimensions W and X returned by PCA are known mathematically as the <span style=\"color:red\">**\"Eigen vectors\"**</span> of the <span style=\"color:red\">**covariance matrix**</span> of the original dimensions which were provided. **This notebook will not address the mathematical aspects and background of the PCA method although very interesting. Further reading on this topic is very widely available online.**<br><br>\n2- The first vector / dimension returned by PCA, \"W\" has an exact slope of 0.5 and corresponds to the direction of the vectors U and V which we had identified \"intuitively\" at the beginning of this notebook. For added clarity, vector V has been added to the drawing above to highlight the fact that it has the exact same slope as W (but a different size and direction).<br><br>\n3- The vector, or dimension X is exactly orthogonal to W. This is a property of the PCA method: all the returned dimensions are orthogonal (even when there are more than two).<br><br>\n4- The coordinates of vectors W and X were not chosen by the Spark PCA at random (i.e we could pick 'longer' or 'shorter' vectors along the same directions). If you calculate the norm (size) of W or X (given by **sqrt(x^2 + y^2)**), you will notice that each one of those vectors has a norm of 1, making them <span style=\"color:blue\">unit</span> vectors. The <span style=\"color:blue\">basis</span> formed by W and X is therefore <span style=\"color:blue\">orthonormal</span>.<br><br>\n5- We can arbitrarily multiply anyone of the PCA dimensions by -1, which would only change the overall direction by 180 degrees, but does not change the orthonormal property.<br><br>\n6- In the <span style=\"color:red\">red</span> orthonormal basis above formed by W and X, we can see that each one of our original datapoints will have a different abscissa (aka X coordinate), but the ordinate (aka Y coordinate) will be rigorously identical for all datapoints. As a matter of fact, if the dataset is initially centered around the origin --imagine shifting the dataset downward in a vertical motion so that datapoint C is at the origin-- (this was performed by the 0 mean transformation of our standard scaler), then we will have the case where the ordinate for each datapoint is <span style=\"color:red\">0</span>.<br><br>\n7- <span style=\"color:red\">We can conclude from the remark at point 6- just above that the second dimension X is not useful at all, and all datapoints can be identified by their abscissa (or X coordinate) along direction W alone. As a matter of fact, if a dataset is to be used as input to a predictive algorithm, if one dimension presents exactly the same value for all datapoints, then it can be dropped as it does not affect the result at all</span>. This knowledge about how much information is provided by each of the PCA dimensions is known as the <span style=\"color:blue\">**Eigen values**</span> of the same covariance matrix discussed in the first point above. The Spark PCA algorithm provides these **Eigen values** associated with each **Eigen vector** (W and X here), through the method **'explainedVariance'** which we will look at below.<br>"}, {"metadata": {}, "cell_type": "markdown", "source": "## &nbsp;&nbsp;&nbsp;<span style=\"color:#fa04d9\">Step 4.2: The \"explainedVariance\" method and the Eigen values:</span>"}, {"metadata": {}, "cell_type": "markdown", "source": "Let us print the Eigen values for the two dimensions W and X studied so far. This is done using the \"explainedVariance\" method:"}, {"metadata": {}, "cell_type": "code", "outputs": [{"metadata": {}, "output_type": "execute_result", "execution_count": 12, "data": {"text/plain": "DenseVector([1.0, 0.0])"}}], "source": "pca_model_instance.explainedVariance", "execution_count": 12}, {"metadata": {}, "cell_type": "markdown", "source": "**If you have followed the analysis of the PCA dimensions (W and X) results described a couple of cells above, you should hopefully not be surprised by the outpout you are seeing. This is telling us that:**<br>\n1- The first principal component, or dimension (Eigen vector W in our case) captures <span style=\"color:red\">**100%**</span> of the variance or information in our dataset.<br>\n2- The second principal component, or dimension (Eigen vector X in our case) caputures <span style=\"color:red\">**0%**</span> of the variance or information in our dataset.<br>\n3- <span style=\"color:red\">**Consequently, we can infer that our dataset is unidimensional and can be fully described with the single FIRST dimension returned by PCA.**</span>"}, {"metadata": {}, "cell_type": "markdown", "source": "## &nbsp;&nbsp;&nbsp;<span style=\"color:#fa04d9\"> Step 4.3: Coordinates of datapoints in the new orthonormal basis returned by PCA </span>"}, {"metadata": {}, "cell_type": "markdown", "source": "We now have two different bases in which we can describe our datapoints A, B, C, D and E. The original basis where all points had an X value and a Y value (two dimensions), and the new (red) basis returned by PCA where we have determined that only one vector is going to be sufficient to describe our dataset. The natural subsequent question then becomes: <span style=\"color:red\">**So how do we find the coordinates of datapoints A, B, C, D and E in the new basis which was returned by PCA (represented in red in the figure higher up)?**</span><br><br>\nSince PCA is a linear transformation of the original dimensions (**details not discussed in this notebook, but details widely available online**), we can obtain the new coordinates of our datapoints in the new basis by multiplying their (original) coordinates by the matrix of Eigen vectors which was discussed a few cells above. You can find in the picture below an example of applying the PCA transformation to the original datapoints A, B, C, D and E\n\n![pca_coords](https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/pictures/pca_coords.jpg)\n\n### <span style=\"color:blue\">Some remarks about the matrix multiplication above:</span><br>\n1- The original datapoints coordinates are collected in the first matrix above, in a transposed (row) format. So point A's coordinates are shown as the first row, point B's coordinates are shown as the second row, and so on...<br><br>\n2- The PCA matrix with the principal components as it was returned by Spark and discussed higher is represented with a more precise version of the coordinates of the Eigen vectors W and X. Note that this matrix is \"vertical\" and the coordinates of W and X are represented \"vertically\".<br><br>\n3- The intermediate resulting matrix shows the details of the multiplication operations which take place, and the final matrix (underneath) shows, in each row, the coordinates of the same datapoints from A to E in the target PCA basis.<br><br>\n4- <span style=\"color:blue\">**As expected, notice how all the datapoints have the same ordinate (Y coordinate) in the new basis, making it therefore useless from a predictive point of view.**</span><br><br>\n5- <span style=\"color:blue\">**One more detail needs to be addressed.**</span> Remember that the coordinates of points A to E were transformed by the standard scaler to a 0 mean set of values. So the initial matrix of coordinates for the datapoints is actually the one returned as the \"scaledFeatures\" higher towards the beginning of this notebook rather than the one shown in the picture above. Here it is below as a reminder:<br>\n![scaled_original_coords](https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/pictures/scaled_original_coords.jpg)\n\n6- If we pass these coordinates through the same matrix multiplication as shown above, we get the following results for the PCA coordinates of points A through E:\n![scaled_pca_coords](https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/pictures/scaled_pca_coords.jpg)"}, {"metadata": {}, "cell_type": "markdown", "source": "### As it turns out, you do not need to perform these matrix multiplications manually. Recall the definition of our PCA model at the beginning of this notebook:\npca_model_definition = PCA(k=2, inputCol=\"scaledFeatures\", <span style=\"color:blue\">**outputCol=\"pcaFeatures\"**</span>)<br><br>\nSpark will actually compute the coordinates of each datapoint in the PCA basis and display them in the output column **pcaFeatures**. Let's take a look at how this works with our example."}, {"metadata": {}, "cell_type": "markdown", "source": "Our PCA model is available as \"pca_model_definition\". We have already looked at invoking \"pc\" and \"explainedVariance\" on our model. As with all Spark models, we can also apply this model to the input dataframe of datapoints (using the \"transform\" method) and obtain as a result, an output dataframe that will have appended to it the new coordinates in the PCA dimensions orthonormal basis."}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "pca_output = pca_model_instance.transform(scaler_output_df)", "execution_count": 13}, {"metadata": {}, "cell_type": "code", "outputs": [{"metadata": {}, "output_type": "execute_result", "execution_count": 14, "data": {"text/plain": "[Row(features=DenseVector([0.0, 1.0]), scaledFeatures=DenseVector([0.0, 0.0]), pcaFeatures=DenseVector([0.0, 0.0])),\n Row(features=DenseVector([2.0, 2.0]), scaledFeatures=DenseVector([2.0, 1.0]), pcaFeatures=DenseVector([-2.2361, 0.0])),\n Row(features=DenseVector([4.0, 3.0]), scaledFeatures=DenseVector([4.0, 2.0]), pcaFeatures=DenseVector([-4.4721, 0.0])),\n Row(features=DenseVector([-2.0, 0.0]), scaledFeatures=DenseVector([-2.0, -1.0]), pcaFeatures=DenseVector([2.2361, 0.0])),\n Row(features=DenseVector([-4.0, -1.0]), scaledFeatures=DenseVector([-4.0, -2.0]), pcaFeatures=DenseVector([4.4721, 0.0]))]"}}], "source": "pca_output.collect()", "execution_count": 14}, {"metadata": {}, "cell_type": "markdown", "source": "### We can see in the cell above:\n- Each one of the original 5 datapoints as a row in the dataframe.\n- Both the original and scaled coordinates before the PCA transformation.\n- The PCA coordinates.\n- <span style=\"color:blue\">We can also compare for correctness (and better understanding) the results in the pcaFeatures column with the results obtained from the manual matrix multiplication a few cells higher (Scaled PCA Coords).</span>"}, {"metadata": {}, "cell_type": "markdown", "source": "# <span style=\"color:#fa04d9\">Step 5: Visual representations using the Python based Brunel library.</span>"}, {"metadata": {}, "cell_type": "markdown", "source": "Now that our understanding of the meaning of the pcaFeatures column is solidifying, we can go one step further and use Python capabilities to plot some data. In order to do this, we will use the Brunel library which is extensively described online. If you are interested in learning more about Brunel, one good starting point would be this PDF document:  http://brunel.mybluemix.net/docs/Brunel%20Documentation.pdf"}, {"metadata": {}, "cell_type": "markdown", "source": "We will be plotting simple two dimensional graphs, where datapoints have X and Y coordinates. One simple input which Brunel can take in order to produce the desired graph is a Pandas dataframe. (If you are not familiar with the Python Pandas library, you can also find several tutorials online).<br><br> The first step will therefore consist in taking the pcaFeatures output provided by Spark right above and extract it as a Pandas dataframe consisting of two columns \"x\" and \"y\"."}, {"metadata": {}, "cell_type": "markdown", "source": "The Spark transformation below is not the most efficient, but represents a straightforward approach to extracting the desired data from the Spark dataframe and creating a Pandas dataframe named \"my_pandas_data\""}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "my_pandas_data = pca_output.select([\"pcaFeatures\"]).rdd.map(lambda row: (float(row[0][0]), float(row[0][1]))).toDF([\"x\", \"y\"]).toPandas()", "execution_count": 15}, {"metadata": {}, "cell_type": "code", "outputs": [{"metadata": {}, "output_type": "execute_result", "execution_count": 16, "data": {"text/html": "<div>\n<style>\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-2.236068</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-4.472136</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.236068</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.472136</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "          x    y\n0  0.000000  0.0\n1 -2.236068  0.0\n2 -4.472136  0.0\n3  2.236068  0.0\n4  4.472136  0.0"}}], "source": "my_pandas_data", "execution_count": 16}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "# Import the brunel library\nimport brunel", "execution_count": 17}, {"metadata": {}, "cell_type": "code", "outputs": [{"metadata": {}, "output_type": "display_data", "data": {"text/html": "<!--\n  ~ Copyright (c) 2015 IBM Corporation and others.\n  ~\n  ~ Licensed under the Apache License, Version 2.0 (the \"License\");\n  ~ You may not use this file except in compliance with the License.\n  ~ You may obtain a copy of the License at\n  ~\n  ~     http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing, software\n  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  ~ See the License for the specific language governing permissions and\n  ~ limitations under the License.\n  -->\n\n\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/data/jupyter2/e1bf8885-4ff9-49a7-a892-aaea0c0192b7/nbextensions/brunel_ext/brunel.2.3.css\">\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/data/jupyter2/e1bf8885-4ff9-49a7-a892-aaea0c0192b7/nbextensions/brunel_ext/sumoselect.css\">\n\n<style>\n    \n</style>\n\n<div id=\"controlside79af9e8-7c71-11e7-ac5b-002590fb6404\" class=\"brunel\"/>\n<svg id=\"viside79af7f4-7c71-11e7-ac5b-002590fb6404\" width=\"800\" height=\"300\"></svg>", "text/plain": "<IPython.core.display.HTML object>"}}, {"metadata": {}, "output_type": "execute_result", "execution_count": 18, "data": {"application/javascript": "/*\n * Copyright (c) 2015 IBM Corporation and others.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * You may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nrequire.config({\n    waitSeconds: 60,\n    paths: {\n        'd3': '//cdnjs.cloudflare.com/ajax/libs/d3/4.2.1/d3.min',\n        'topojson': '//cdnjs.cloudflare.com/ajax/libs/topojson/1.6.20/topojson.min',\n        'brunel' : '/data/jupyter2/e1bf8885-4ff9-49a7-a892-aaea0c0192b7/nbextensions/brunel_ext/brunel.2.3.min',\n        'brunelControls' : '/data/jupyter2/e1bf8885-4ff9-49a7-a892-aaea0c0192b7/nbextensions/brunel_ext/brunel.controls.2.3.min'\n    },\n    shim: {\n       'brunel' : {\n            exports: 'BrunelD3',\n            deps: ['d3', 'topojson'],\n            init: function() {\n               return {\n                 BrunelD3 : BrunelD3,\n                 BrunelData : BrunelData\n              }\n            }\n        },\n       'brunelControls' : {\n            exports: 'BrunelEventHandlers',\n            init: function() {\n               return {\n                 BrunelEventHandlers: BrunelEventHandlers,\n                 BrunelJQueryControlFactory: BrunelJQueryControlFactory\n              }\n            }\n        }\n\n    }\n\n});\n\nrequire([\"d3\"], function(d3) {\n    require([\"brunel\", \"brunelControls\"], function(brunel, brunelControls) {\n        function  BrunelVis(visId) {\n  \"use strict\";                                                                       // strict mode\n  var datasets = [],                                      // array of datasets for the original data\n      pre = function(d, i) { return d },                         // default pre-process does nothing\n      post = function(d, i) { return d },                       // default post-process does nothing\n      transitionTime = 200,                                        // transition time for animations\n      charts = [],                                                       // the charts in the system\n      vis = d3.select('#' + visId).attr('class', 'brunel');                     // the SVG container\n\n  BrunelD3.addDefinitions(vis);                                   // ensure standard symbols present\n\n  // Define chart #1 in the visualization //////////////////////////////////////////////////////////\n\n  charts[0] = function(parentNode, filterRows) {\n    var geom = BrunelD3.geometry(parentNode || vis.node(), 0, 0, 1, 1, 5, 43, 37, 13),\n      elements = [];                                              // array of elements in this chart\n\n    // Define groups for the chart parts ///////////////////////////////////////////////////////////\n\n    var chart =  vis.append('g').attr('class', 'chart1')\n      .attr('transform','translate(' + geom.chart_left + ',' + geom.chart_top + ')');\n    var overlay = chart.append('g').attr('class', 'element').attr('class', 'overlay');\n    var zoom = d3.zoom().scaleExtent([1/3,3]);\n    var zoomNode = overlay.append('rect').attr('class', 'overlay')\n      .attr('x', geom.inner_left).attr('y', geom.inner_top)\n      .attr('width', geom.inner_rawWidth).attr('height', geom.inner_rawHeight)\n      .style('cursor', 'move').call(zoom)\n      .node();\n    zoomNode.__zoom = d3.zoomIdentity;\n    chart.append('rect').attr('class', 'background').attr('width', geom.chart_right-geom.chart_left).attr('height', geom.chart_bottom-geom.chart_top);\n    var interior = chart.append('g').attr('class', 'interior zoomNone')\n      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')')\n      .attr('clip-path', 'url(#clip_viside79af7f4-7c71-11e7-ac5b-002590fb6404_chart1_inner)');\n    interior.append('rect').attr('class', 'inner').attr('width', geom.inner_width).attr('height', geom.inner_height);\n    var gridGroup = interior.append('g').attr('class', 'grid');\n    var axes = chart.append('g').attr('class', 'axis')\n      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')');\n    vis.append('clipPath').attr('id', 'clip_viside79af7f4-7c71-11e7-ac5b-002590fb6404_chart1_inner').append('rect')\n      .attr('x', 0).attr('y', 0)\n      .attr('width', geom.inner_rawWidth+1).attr('height', geom.inner_rawHeight+1);\n\n    // Scales //////////////////////////////////////////////////////////////////////////////////////\n\n    var scale_x = d3.scaleLinear().domain([-5, 5.000001])\n      .range([0, geom.inner_width]);\n    var scale_inner = d3.scaleLinear().domain([0,1])\n      .range([-0.5, 0.5]);\n    var scale_y = d3.scaleLinear().domain([0, 1.0000001])\n      .range([geom.inner_height, 0]);\n    var base_scales = [scale_x, scale_y];                           // untransformed original scales\n\n    // Axes ////////////////////////////////////////////////////////////////////////////////////////\n\n    axes.append('g').attr('class', 'x axis')\n      .attr('transform','translate(0,' + geom.inner_rawHeight + ')')\n      .attr('clip-path', 'url(#clip_viside79af7f4-7c71-11e7-ac5b-002590fb6404_chart1_haxis)');\n    vis.append('clipPath').attr('id', 'clip_viside79af7f4-7c71-11e7-ac5b-002590fb6404_chart1_haxis').append('polyline')\n      .attr('points', '-1,-1000, -1,-1 -5,5, -1000,5, -100,1000, 10000,1000 10000,-1000');\n    axes.select('g.axis.x').append('text').attr('class', 'title').text('X').style('text-anchor', 'middle')\n      .attr('x',geom.inner_rawWidth/2)\n      .attr('y', geom.inner_bottom - 2.0).attr('dy','-0.27em');\n    axes.append('g').attr('class', 'y axis')\n      .attr('clip-path', 'url(#clip_viside79af7f4-7c71-11e7-ac5b-002590fb6404_chart1_vaxis)');\n    vis.append('clipPath').attr('id', 'clip_viside79af7f4-7c71-11e7-ac5b-002590fb6404_chart1_vaxis').append('polyline')\n      .attr('points', '-1000,-10000, 10000,-10000, 10000,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+5) + ', -1000,' + (geom.inner_rawHeight+5) );\n    axes.select('g.axis.y').append('text').attr('class', 'title').text('Y').style('text-anchor', 'middle')\n      .attr('x',-geom.inner_rawHeight/2)\n      .attr('y', 4-geom.inner_left).attr('dy', '0.7em').attr('transform', 'rotate(270)');\n\n    var axis_bottom = d3.axisBottom(scale_x).ticks(Math.min(10, Math.round(geom.inner_width / 33.0)));\n    var axis_left = d3.axisLeft(scale_y).ticks(Math.min(10, Math.round(geom.inner_width / 20)));\n\n    function buildAxes(time) {\n      var axis_x = axes.select('g.axis.x');\n      BrunelD3.transition(axis_x, time).call(axis_bottom.scale(scale_x));\n      var axis_y = axes.select('g.axis.y');\n      BrunelD3.transition(axis_y, time).call(axis_left.scale(scale_y));\n    }\n    zoom.on('zoom', function(t, time) {\n        t = t ||BrunelD3.restrictZoom(d3.event.transform, geom, this);\n        scale_x = t.rescaleX(base_scales[0]);\n        scale_y = t.rescaleY(base_scales[1]);\n        zoomNode.__zoom = t;\n        interior.attr('class', 'interior ' + BrunelD3.zoomLabel(t.k));;\n        build(time || -1);\n    });\n\n    // Define element #1 ///////////////////////////////////////////////////////////////////////////\n\n    elements[0] = function() {\n      var original, processed,                           // data sets passed in and then transformed\n        element, data,                                 // brunel element information and brunel data\n        selection, merged;                                      // d3 selection and merged selection\n      var elementGroup = interior.append('g').attr('class', 'element1'),\n        main = elementGroup.append('g').attr('class', 'main'),\n        labels = BrunelD3.undoTransform(elementGroup.append('g').attr('class', 'labels').attr('aria-hidden', 'true'), elementGroup);\n\n      function makeData() {\n        original = datasets[0];\n        if (filterRows) original = original.retainRows(filterRows);\n        processed = pre(original, 0);\n        processed = post(processed, 0);\n        var f0 = processed.field('x'),\n          f1 = processed.field('y'),\n          f2 = processed.field('#selection'),\n          f3 = processed.field('#row');\n        var keyFunc = function(d) { return f3.value(d) };\n        data = {\n          x:            function(d) { return f0.value(d.row) },\n          y:            function(d) { return f1.value(d.row) },\n          $selection:   function(d) { return f2.value(d.row) },\n          $row:         function(d) { return f3.value(d.row) },\n          x_f:          function(d) { return f0.valueFormatted(d.row) },\n          y_f:          function(d) { return f1.valueFormatted(d.row) },\n          $selection_f: function(d) { return f2.valueFormatted(d.row) },\n          $row_f:       function(d) { return f3.valueFormatted(d.row) },\n          _split:       function(d) { return f2.value(d.row) },\n          _key:         keyFunc,\n          _rows:        BrunelD3.makeRowsWithKeys(keyFunc, processed.rowCount())\n        };\n      }\n      // Aesthetic Functions\n      var scale_color = d3.scaleOrdinal()\n        .domain(['\u2717', '\u2713'])\n        .range([ '#00538A', '#C10020', '#F4C800', '#007D34', '#803E75', '#FF6800', \n          '#817066', '#FFB300', '#F6768E', '#93AA00', '#53377A', '#FF8E00', '#B32851', \n          '#CEA262', '#FF7A5C', '#7F180D', '#593315', '#F13A13', '#232C16']);\n      var color = function(d) { return scale_color(data.$selection(d)) };\n\n      // Build element from data ///////////////////////////////////////////////////////////////////\n\n      function build(transitionMillis) {\n        element = elements[0];\n        var w = Math.abs( scale_x(scale_x.domain()[0] + 2.2360679774997894) - scale_x.range()[0] );\n        var x = function(d) { return scale_x(data.x(d))};\n        var h = geom.default_point_size;\n        var y = function(d) { return scale_y(data.y(d))};\n\n        // Define selection entry operations\n        function initialState(selection) {\n          selection\n            .attr('class', 'element point filled')\n            .style('pointer-events', 'none')\n        }\n\n        // Define selection update operations on merged data\n        function updateState(selection) {\n          selection\n            .attr('cx',function(d) { return scale_x(data.x(d))})\n            .attr('cy',function(d) { return scale_y(data.y(d))})\n            .attr('r',Math.min(Math.abs( scale_x(scale_x.domain()[0] + 2.2360679774997894) - scale_x.range()[0] ), geom.default_point_size) / 2)\n            .filter(BrunelD3.hasData)                     // following only performed for data items\n            .style('fill', color);\n        }\n\n        // Define labeling for the selection\n        function label(selection, transitionMillis) {\n        }\n        // Create selections, set the initial state and transition updates\n        selection = main.selectAll('.element').data(data._rows, function(d) { return d.key });\n        var added = selection.enter().append('circle');\n        merged = selection.merge(added);\n        initialState(added);\n        selection.filter(BrunelD3.hasData)\n          .classed('selected', BrunelD3.isSelected(data))\n          .filter(BrunelD3.isSelected(data)).raise();\n        updateState(BrunelD3.transition(merged, transitionMillis));\n\n        BrunelD3.transition(selection.exit(), transitionMillis/3)\n          .style('opacity', 0.5).each( function() {\n            this.remove(); BrunelD3.removeLabels(this); \n        });\n      }\n\n      return {\n        data:           function() { return processed },\n        original:       function() { return original },\n        internal:       function() { return data },\n        selection:      function() { return merged },\n        makeData:       makeData,\n        build:          build,\n        chart:          function() { return charts[0] },\n        group:          function() { return elementGroup },\n        fields: {\n          x:            ['x'],\n          y:            ['y'],\n          key:          ['#row'],\n          color:        ['#selection']\n        }\n      };\n    }();\n\n    function build(time, noData) {\n      var first = elements[0].data() == null;\n      if (first) time = 0;                                           // no transition for first call\n      buildAxes(time);\n      if ((first || time > -1) && !noData) {\n        elements[0].makeData();\n      }\n      elements[0].build(time);\n    }\n\n    // Expose the following components of the chart\n    return {\n      elements : elements,\n      interior : interior,\n      scales: {x:scale_x, y:scale_y},\n      zoom: function(params, time) {\n          if (params) zoom.on('zoom').call(zoomNode, params, time);\n          return d3.zoomTransform(zoomNode);\n      },\n      build : build\n    };\n    }();\n\n  function setData(rowData, i) { datasets[i||0] = BrunelD3.makeData(rowData) }\n  function updateAll(time) { charts.forEach(function(x) {x.build(time || 0)}) }\n  function buildAll() {\n    for (var i=0;i<arguments.length;i++) setData(arguments[i], i);\n    updateAll(transitionTime);\n  }\n\n  return {\n    dataPreProcess:     function(f) { if (f) pre = f; return pre },\n    dataPostProcess:    function(f) { if (f) post = f; return post },\n    data:               function(d,i) { if (d) setData(d,i); return datasets[i||0] },\n    visId:              visId,\n    build:              buildAll,\n    rebuild:            updateAll,\n    charts:             charts\n  }\n}\n\n// Data Tables /////////////////////////////////////////////////////////////////////////////////////\n\nvar table1 = {\n   summarized: false,\n   names: ['x', 'y'], \n   options: ['numeric', 'numeric'], \n   rows: [[0, 0], [-2.236068, 0], [-4.472136, 0], [2.236068, 0], [4.472136, 0]]\n};\n\n// Call Code to Build the system ///////////////////////////////////////////////////////////////////\n\nvar v  = new BrunelVis('viside79af7f4-7c71-11e7-ac5b-002590fb6404');\nv.build(table1);\n\n    });\n});", "text/plain": "<IPython.core.display.Javascript object>"}}], "source": "# Plot the 5 datapoints\n%brunel data('my_pandas_data') point x(x) y(y) color(#selection):: width=800, height=300", "execution_count": 18}, {"metadata": {}, "cell_type": "markdown", "source": "# Conclusion:<br> \nWe have seen in this first simple example how PCA can be used to reduce the dimensionality of a problem / dataset. In this example above, if the initial dataset was to be used as input to a predictive algorithm, we could reduce the input to a single dimension through PCA"}, {"metadata": {}, "cell_type": "markdown", "source": "# <span style=\"color:#fa04d9\">Step 6: 2D exercise.</span>"}, {"metadata": {}, "cell_type": "markdown", "source": "<span style=\"color:blue\">**Insert a few blank cells below in this notebook and modify your datapoints in such a way that the resulting dataset is not aligned anymore and then rerun the principal component analysis as shown above (including the brunel visualizations)**:\n- Which significant change do you anticipate to observe in comparison with the example covered so far ?\n- How many dimensions do you need to describe your new dataset ? \n- Would you still keep one single dimension if your data was being used for predictive purposes ? Why or Why not ?\n- Using brunel, plot your original data and the transformation into the PCA dimension(s)\n- Discuss your results with you neighbor or instructor at your preference.\n- **Stretch goal: Keep modifying your dataset in specific ways, such as giving it a particular shape (elongated in one particular direction, etc...) and keep running PCA and check how the algorithm will systematically extract the first dimension as the direction in which your dataset has the most information.****</span>"}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "Here is one suggested modified dataset to get started."}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "data = [(Vectors.dense([-4.0, -2.0]),),\n        (Vectors.dense([-2.0, 0.5]),),\n        (Vectors.dense([1.0, 1.0]),),\n        (Vectors.dense([1.0, -2.0]),),\n        (Vectors.dense([2.0, 0.0]),),\n        (Vectors.dense([4.0, 2.0]),)\n        ]\n\ndf = spark.createDataFrame(data, [\"features\"])", "execution_count": 19}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "scaler_definition = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n                        withStd=False, withMean=True)", "execution_count": 20}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "scaler_instance_trained = scaler_definition.fit(df)", "execution_count": 21}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "scaler_output_df = scaler_instance_trained.transform(df)", "execution_count": 22}, {"metadata": {}, "cell_type": "code", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------+--------------------+\n|   features|      scaledFeatures|\n+-----------+--------------------+\n|[-4.0,-2.0]|[-4.3333333333333...|\n| [-2.0,0.5]|[-2.3333333333333...|\n|  [1.0,1.0]|[0.66666666666666...|\n| [1.0,-2.0]|[0.66666666666666...|\n|  [2.0,0.0]|[1.66666666666666...|\n|  [4.0,2.0]|[3.66666666666666...|\n+-----------+--------------------+\n\n"}], "source": "scaler_output_df.show()", "execution_count": 23}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "pca = PCA(k=2, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")", "execution_count": 24}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "pca_model_definition = pca.fit(scaler_output_df)", "execution_count": 25}, {"metadata": {}, "cell_type": "code", "outputs": [{"metadata": {}, "output_type": "execute_result", "execution_count": 26, "data": {"text/plain": "DenseMatrix(2, 2, [-0.9232, -0.3844, -0.3844, 0.9232], 0)"}}], "source": "pca_model_definition.pc", "execution_count": 26}, {"metadata": {}, "cell_type": "code", "outputs": [{"metadata": {}, "output_type": "execute_result", "execution_count": 27, "data": {"text/plain": "DenseVector([0.866, 0.134])"}}], "source": "pca_model_definition.explainedVariance", "execution_count": 27}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "pca_output = pca_model_instance.transform(scaler_output_df)", "execution_count": 28}, {"metadata": {}, "cell_type": "markdown", "source": "Convert the PCA output to a pandas dataframe"}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "pca_pandas = pca_output.select([\"pcaFeatures\"]).rdd.map(lambda row: (float(row[0][0]), float(row[0][1]))).toDF([\"x\", \"y\"]).toPandas()", "execution_count": 29}, {"metadata": {}, "cell_type": "code", "outputs": [{"metadata": {}, "output_type": "execute_result", "execution_count": 30, "data": {"text/html": "<div>\n<style>\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.733011</td>\n      <td>0.223607</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.826122</td>\n      <td>1.565248</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.080766</td>\n      <td>0.670820</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.260875</td>\n      <td>-2.012461</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.527980</td>\n      <td>-0.670820</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-4.211261</td>\n      <td>0.223607</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "          x         y\n0  4.733011  0.223607\n1  1.826122  1.565248\n2 -1.080766  0.670820\n3  0.260875 -2.012461\n4 -1.527980 -0.670820\n5 -4.211261  0.223607"}}], "source": "pca_pandas", "execution_count": 30}, {"metadata": {}, "cell_type": "markdown", "source": "Convert the original dataset to a pandas dataframe"}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "df_panda = df.select([\"features\"]).rdd.map(lambda row: (float(row[0][0]), float(row[0][1]))).toDF([\"x\", \"y\"]).toPandas()", "execution_count": 31}, {"metadata": {}, "cell_type": "code", "outputs": [{"metadata": {}, "output_type": "execute_result", "execution_count": 32, "data": {"text/html": "<div>\n<style>\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-4.0</td>\n      <td>-2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-2.0</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>-2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4.0</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "     x    y\n0 -4.0 -2.0\n1 -2.0  0.5\n2  1.0  1.0\n3  1.0 -2.0\n4  2.0  0.0\n5  4.0  2.0"}}], "source": "df_panda", "execution_count": 32}, {"metadata": {}, "cell_type": "markdown", "source": "Plot the original dataset using Brunel"}, {"metadata": {}, "cell_type": "code", "outputs": [{"metadata": {}, "output_type": "display_data", "data": {"text/html": "<!--\n  ~ Copyright (c) 2015 IBM Corporation and others.\n  ~\n  ~ Licensed under the Apache License, Version 2.0 (the \"License\");\n  ~ You may not use this file except in compliance with the License.\n  ~ You may obtain a copy of the License at\n  ~\n  ~     http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing, software\n  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  ~ See the License for the specific language governing permissions and\n  ~ limitations under the License.\n  -->\n\n\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/data/jupyter2/e1bf8885-4ff9-49a7-a892-aaea0c0192b7/nbextensions/brunel_ext/brunel.2.3.css\">\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/data/jupyter2/e1bf8885-4ff9-49a7-a892-aaea0c0192b7/nbextensions/brunel_ext/sumoselect.css\">\n\n<style>\n    \n</style>\n\n<div id=\"controlside8c87840-7c71-11e7-ac5b-002590fb6404\" class=\"brunel\"/>\n<svg id=\"viside8c876c4-7c71-11e7-ac5b-002590fb6404\" width=\"800\" height=\"500\"></svg>", "text/plain": "<IPython.core.display.HTML object>"}}, {"metadata": {}, "output_type": "execute_result", "execution_count": 33, "data": {"application/javascript": "/*\n * Copyright (c) 2015 IBM Corporation and others.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * You may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nrequire.config({\n    waitSeconds: 60,\n    paths: {\n        'd3': '//cdnjs.cloudflare.com/ajax/libs/d3/4.2.1/d3.min',\n        'topojson': '//cdnjs.cloudflare.com/ajax/libs/topojson/1.6.20/topojson.min',\n        'brunel' : '/data/jupyter2/e1bf8885-4ff9-49a7-a892-aaea0c0192b7/nbextensions/brunel_ext/brunel.2.3.min',\n        'brunelControls' : '/data/jupyter2/e1bf8885-4ff9-49a7-a892-aaea0c0192b7/nbextensions/brunel_ext/brunel.controls.2.3.min'\n    },\n    shim: {\n       'brunel' : {\n            exports: 'BrunelD3',\n            deps: ['d3', 'topojson'],\n            init: function() {\n               return {\n                 BrunelD3 : BrunelD3,\n                 BrunelData : BrunelData\n              }\n            }\n        },\n       'brunelControls' : {\n            exports: 'BrunelEventHandlers',\n            init: function() {\n               return {\n                 BrunelEventHandlers: BrunelEventHandlers,\n                 BrunelJQueryControlFactory: BrunelJQueryControlFactory\n              }\n            }\n        }\n\n    }\n\n});\n\nrequire([\"d3\"], function(d3) {\n    require([\"brunel\", \"brunelControls\"], function(brunel, brunelControls) {\n        function  BrunelVis(visId) {\n  \"use strict\";                                                                       // strict mode\n  var datasets = [],                                      // array of datasets for the original data\n      pre = function(d, i) { return d },                         // default pre-process does nothing\n      post = function(d, i) { return d },                       // default post-process does nothing\n      transitionTime = 200,                                        // transition time for animations\n      charts = [],                                                       // the charts in the system\n      vis = d3.select('#' + visId).attr('class', 'brunel');                     // the SVG container\n\n  BrunelD3.addDefinitions(vis);                                   // ensure standard symbols present\n\n  // Define chart #1 in the visualization //////////////////////////////////////////////////////////\n\n  charts[0] = function(parentNode, filterRows) {\n    var geom = BrunelD3.geometry(parentNode || vis.node(), 0, 0, 1, 1, 5, 43, 37, 13),\n      elements = [];                                              // array of elements in this chart\n\n    // Define groups for the chart parts ///////////////////////////////////////////////////////////\n\n    var chart =  vis.append('g').attr('class', 'chart1')\n      .attr('transform','translate(' + geom.chart_left + ',' + geom.chart_top + ')');\n    var overlay = chart.append('g').attr('class', 'element').attr('class', 'overlay');\n    var zoom = d3.zoom().scaleExtent([1/3,3]);\n    var zoomNode = overlay.append('rect').attr('class', 'overlay')\n      .attr('x', geom.inner_left).attr('y', geom.inner_top)\n      .attr('width', geom.inner_rawWidth).attr('height', geom.inner_rawHeight)\n      .style('cursor', 'move').call(zoom)\n      .node();\n    zoomNode.__zoom = d3.zoomIdentity;\n    chart.append('rect').attr('class', 'background').attr('width', geom.chart_right-geom.chart_left).attr('height', geom.chart_bottom-geom.chart_top);\n    var interior = chart.append('g').attr('class', 'interior zoomNone')\n      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')')\n      .attr('clip-path', 'url(#clip_viside8c876c4-7c71-11e7-ac5b-002590fb6404_chart1_inner)');\n    interior.append('rect').attr('class', 'inner').attr('width', geom.inner_width).attr('height', geom.inner_height);\n    var gridGroup = interior.append('g').attr('class', 'grid');\n    var axes = chart.append('g').attr('class', 'axis')\n      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')');\n    vis.append('clipPath').attr('id', 'clip_viside8c876c4-7c71-11e7-ac5b-002590fb6404_chart1_inner').append('rect')\n      .attr('x', 0).attr('y', 0)\n      .attr('width', geom.inner_rawWidth+1).attr('height', geom.inner_rawHeight+1);\n\n    // Scales //////////////////////////////////////////////////////////////////////////////////////\n\n    var scale_x = d3.scaleLinear().domain([-5, 5.000001])\n      .range([0, geom.inner_width]);\n    var scale_inner = d3.scaleLinear().domain([0,1])\n      .range([-0.5, 0.5]);\n    var scale_y = d3.scaleLinear().domain([-2.5, 2.5000005])\n      .range([geom.inner_height, 0]);\n    var base_scales = [scale_x, scale_y];                           // untransformed original scales\n\n    // Axes ////////////////////////////////////////////////////////////////////////////////////////\n\n    axes.append('g').attr('class', 'x axis')\n      .attr('transform','translate(0,' + geom.inner_rawHeight + ')')\n      .attr('clip-path', 'url(#clip_viside8c876c4-7c71-11e7-ac5b-002590fb6404_chart1_haxis)');\n    vis.append('clipPath').attr('id', 'clip_viside8c876c4-7c71-11e7-ac5b-002590fb6404_chart1_haxis').append('polyline')\n      .attr('points', '-1,-1000, -1,-1 -5,5, -1000,5, -100,1000, 10000,1000 10000,-1000');\n    axes.select('g.axis.x').append('text').attr('class', 'title').text('X').style('text-anchor', 'middle')\n      .attr('x',geom.inner_rawWidth/2)\n      .attr('y', geom.inner_bottom - 2.0).attr('dy','-0.27em');\n    axes.append('g').attr('class', 'y axis')\n      .attr('clip-path', 'url(#clip_viside8c876c4-7c71-11e7-ac5b-002590fb6404_chart1_vaxis)');\n    vis.append('clipPath').attr('id', 'clip_viside8c876c4-7c71-11e7-ac5b-002590fb6404_chart1_vaxis').append('polyline')\n      .attr('points', '-1000,-10000, 10000,-10000, 10000,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+5) + ', -1000,' + (geom.inner_rawHeight+5) );\n    axes.select('g.axis.y').append('text').attr('class', 'title').text('Y').style('text-anchor', 'middle')\n      .attr('x',-geom.inner_rawHeight/2)\n      .attr('y', 4-geom.inner_left).attr('dy', '0.7em').attr('transform', 'rotate(270)');\n\n    var axis_bottom = d3.axisBottom(scale_x).ticks(Math.min(10, Math.round(geom.inner_width / 33.0)));\n    var axis_left = d3.axisLeft(scale_y).ticks(Math.min(10, Math.round(geom.inner_width / 20)));\n\n    function buildAxes(time) {\n      var axis_x = axes.select('g.axis.x');\n      BrunelD3.transition(axis_x, time).call(axis_bottom.scale(scale_x));\n      var axis_y = axes.select('g.axis.y');\n      BrunelD3.transition(axis_y, time).call(axis_left.scale(scale_y));\n    }\n    zoom.on('zoom', function(t, time) {\n        t = t ||BrunelD3.restrictZoom(d3.event.transform, geom, this);\n        scale_x = t.rescaleX(base_scales[0]);\n        scale_y = t.rescaleY(base_scales[1]);\n        zoomNode.__zoom = t;\n        interior.attr('class', 'interior ' + BrunelD3.zoomLabel(t.k));;\n        build(time || -1);\n    });\n\n    // Define element #1 ///////////////////////////////////////////////////////////////////////////\n\n    elements[0] = function() {\n      var original, processed,                           // data sets passed in and then transformed\n        element, data,                                 // brunel element information and brunel data\n        selection, merged;                                      // d3 selection and merged selection\n      var elementGroup = interior.append('g').attr('class', 'element1'),\n        main = elementGroup.append('g').attr('class', 'main'),\n        labels = BrunelD3.undoTransform(elementGroup.append('g').attr('class', 'labels').attr('aria-hidden', 'true'), elementGroup);\n\n      function makeData() {\n        original = datasets[0];\n        if (filterRows) original = original.retainRows(filterRows);\n        processed = pre(original, 0);\n        processed = post(processed, 0);\n        var f0 = processed.field('x'),\n          f1 = processed.field('y'),\n          f2 = processed.field('#selection'),\n          f3 = processed.field('#row');\n        var keyFunc = function(d) { return f3.value(d) };\n        data = {\n          x:            function(d) { return f0.value(d.row) },\n          y:            function(d) { return f1.value(d.row) },\n          $selection:   function(d) { return f2.value(d.row) },\n          $row:         function(d) { return f3.value(d.row) },\n          x_f:          function(d) { return f0.valueFormatted(d.row) },\n          y_f:          function(d) { return f1.valueFormatted(d.row) },\n          $selection_f: function(d) { return f2.valueFormatted(d.row) },\n          $row_f:       function(d) { return f3.valueFormatted(d.row) },\n          _split:       function(d) { return f2.value(d.row) },\n          _key:         keyFunc,\n          _rows:        BrunelD3.makeRowsWithKeys(keyFunc, processed.rowCount())\n        };\n      }\n      // Aesthetic Functions\n      var scale_color = d3.scaleOrdinal()\n        .domain(['\u2717', '\u2713'])\n        .range([ '#00538A', '#C10020', '#F4C800', '#007D34', '#803E75', '#FF6800', \n          '#817066', '#FFB300', '#F6768E', '#93AA00', '#53377A', '#FF8E00', '#B32851', \n          '#CEA262', '#FF7A5C', '#7F180D', '#593315', '#F13A13', '#232C16']);\n      var color = function(d) { return scale_color(data.$selection(d)) };\n\n      // Build element from data ///////////////////////////////////////////////////////////////////\n\n      function build(transitionMillis) {\n        element = elements[0];\n        var w = Math.abs( scale_x(scale_x.domain()[0] + 1.0) - scale_x.range()[0] );\n        var x = function(d) { return scale_x(data.x(d))};\n        var h = Math.abs( scale_y(scale_y.domain()[0] + 0.5) - scale_y.range()[0] );\n        var y = function(d) { return scale_y(data.y(d))};\n\n        // Define selection entry operations\n        function initialState(selection) {\n          selection\n            .attr('class', 'element point filled')\n            .style('pointer-events', 'none')\n        }\n\n        // Define selection update operations on merged data\n        function updateState(selection) {\n          selection\n            .attr('cx',function(d) { return scale_x(data.x(d))})\n            .attr('cy',function(d) { return scale_y(data.y(d))})\n            .attr('r',Math.min(Math.abs( scale_x(scale_x.domain()[0] + 1.0) - scale_x.range()[0] ), Math.abs( scale_y(scale_y.domain()[0] + 0.5) - scale_y.range()[0] )) / 2)\n            .filter(BrunelD3.hasData)                     // following only performed for data items\n            .style('fill', color);\n        }\n\n        // Define labeling for the selection\n        function label(selection, transitionMillis) {\n        }\n        // Create selections, set the initial state and transition updates\n        selection = main.selectAll('.element').data(data._rows, function(d) { return d.key });\n        var added = selection.enter().append('circle');\n        merged = selection.merge(added);\n        initialState(added);\n        selection.filter(BrunelD3.hasData)\n          .classed('selected', BrunelD3.isSelected(data))\n          .filter(BrunelD3.isSelected(data)).raise();\n        updateState(BrunelD3.transition(merged, transitionMillis));\n\n        BrunelD3.transition(selection.exit(), transitionMillis/3)\n          .style('opacity', 0.5).each( function() {\n            this.remove(); BrunelD3.removeLabels(this); \n        });\n      }\n\n      return {\n        data:           function() { return processed },\n        original:       function() { return original },\n        internal:       function() { return data },\n        selection:      function() { return merged },\n        makeData:       makeData,\n        build:          build,\n        chart:          function() { return charts[0] },\n        group:          function() { return elementGroup },\n        fields: {\n          x:            ['x'],\n          y:            ['y'],\n          key:          ['#row'],\n          color:        ['#selection']\n        }\n      };\n    }();\n\n    function build(time, noData) {\n      var first = elements[0].data() == null;\n      if (first) time = 0;                                           // no transition for first call\n      buildAxes(time);\n      if ((first || time > -1) && !noData) {\n        elements[0].makeData();\n      }\n      elements[0].build(time);\n    }\n\n    // Expose the following components of the chart\n    return {\n      elements : elements,\n      interior : interior,\n      scales: {x:scale_x, y:scale_y},\n      zoom: function(params, time) {\n          if (params) zoom.on('zoom').call(zoomNode, params, time);\n          return d3.zoomTransform(zoomNode);\n      },\n      build : build\n    };\n    }();\n\n  function setData(rowData, i) { datasets[i||0] = BrunelD3.makeData(rowData) }\n  function updateAll(time) { charts.forEach(function(x) {x.build(time || 0)}) }\n  function buildAll() {\n    for (var i=0;i<arguments.length;i++) setData(arguments[i], i);\n    updateAll(transitionTime);\n  }\n\n  return {\n    dataPreProcess:     function(f) { if (f) pre = f; return pre },\n    dataPostProcess:    function(f) { if (f) post = f; return post },\n    data:               function(d,i) { if (d) setData(d,i); return datasets[i||0] },\n    visId:              visId,\n    build:              buildAll,\n    rebuild:            updateAll,\n    charts:             charts\n  }\n}\n\n// Data Tables /////////////////////////////////////////////////////////////////////////////////////\n\nvar table1 = {\n   summarized: false,\n   names: ['x', 'y'], \n   options: ['numeric', 'numeric'], \n   rows: [[-4, -2], [-2, 0.5], [1, 1], [1, -2], [2, 0], [4, 2]]\n};\n\n// Call Code to Build the system ///////////////////////////////////////////////////////////////////\n\nvar v  = new BrunelVis('viside8c876c4-7c71-11e7-ac5b-002590fb6404');\nv.build(table1);\n\n    });\n});", "text/plain": "<IPython.core.display.Javascript object>"}}], "source": "%brunel data('df_panda') x(x) y(y) color(#selection) :: width=800, height=500", "execution_count": 33}, {"metadata": {}, "cell_type": "markdown", "source": "Plot the PCA output using Brunel"}, {"metadata": {}, "cell_type": "code", "outputs": [{"metadata": {}, "output_type": "display_data", "data": {"text/html": "<!--\n  ~ Copyright (c) 2015 IBM Corporation and others.\n  ~\n  ~ Licensed under the Apache License, Version 2.0 (the \"License\");\n  ~ You may not use this file except in compliance with the License.\n  ~ You may obtain a copy of the License at\n  ~\n  ~     http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing, software\n  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  ~ See the License for the specific language governing permissions and\n  ~ limitations under the License.\n  -->\n\n\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/data/jupyter2/e1bf8885-4ff9-49a7-a892-aaea0c0192b7/nbextensions/brunel_ext/brunel.2.3.css\">\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/data/jupyter2/e1bf8885-4ff9-49a7-a892-aaea0c0192b7/nbextensions/brunel_ext/sumoselect.css\">\n\n<style>\n    \n</style>\n\n<div id=\"controlside8cab92a-7c71-11e7-ac5b-002590fb6404\" class=\"brunel\"/>\n<svg id=\"viside8cab7c2-7c71-11e7-ac5b-002590fb6404\" width=\"800\" height=\"500\"></svg>", "text/plain": "<IPython.core.display.HTML object>"}}, {"metadata": {}, "output_type": "execute_result", "execution_count": 34, "data": {"application/javascript": "/*\n * Copyright (c) 2015 IBM Corporation and others.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * You may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nrequire.config({\n    waitSeconds: 60,\n    paths: {\n        'd3': '//cdnjs.cloudflare.com/ajax/libs/d3/4.2.1/d3.min',\n        'topojson': '//cdnjs.cloudflare.com/ajax/libs/topojson/1.6.20/topojson.min',\n        'brunel' : '/data/jupyter2/e1bf8885-4ff9-49a7-a892-aaea0c0192b7/nbextensions/brunel_ext/brunel.2.3.min',\n        'brunelControls' : '/data/jupyter2/e1bf8885-4ff9-49a7-a892-aaea0c0192b7/nbextensions/brunel_ext/brunel.controls.2.3.min'\n    },\n    shim: {\n       'brunel' : {\n            exports: 'BrunelD3',\n            deps: ['d3', 'topojson'],\n            init: function() {\n               return {\n                 BrunelD3 : BrunelD3,\n                 BrunelData : BrunelData\n              }\n            }\n        },\n       'brunelControls' : {\n            exports: 'BrunelEventHandlers',\n            init: function() {\n               return {\n                 BrunelEventHandlers: BrunelEventHandlers,\n                 BrunelJQueryControlFactory: BrunelJQueryControlFactory\n              }\n            }\n        }\n\n    }\n\n});\n\nrequire([\"d3\"], function(d3) {\n    require([\"brunel\", \"brunelControls\"], function(brunel, brunelControls) {\n        function  BrunelVis(visId) {\n  \"use strict\";                                                                       // strict mode\n  var datasets = [],                                      // array of datasets for the original data\n      pre = function(d, i) { return d },                         // default pre-process does nothing\n      post = function(d, i) { return d },                       // default post-process does nothing\n      transitionTime = 200,                                        // transition time for animations\n      charts = [],                                                       // the charts in the system\n      vis = d3.select('#' + visId).attr('class', 'brunel');                     // the SVG container\n\n  BrunelD3.addDefinitions(vis);                                   // ensure standard symbols present\n\n  // Define chart #1 in the visualization //////////////////////////////////////////////////////////\n\n  charts[0] = function(parentNode, filterRows) {\n    var geom = BrunelD3.geometry(parentNode || vis.node(), 0, 0, 1, 1, 5, 43, 37, 13),\n      elements = [];                                              // array of elements in this chart\n\n    // Define groups for the chart parts ///////////////////////////////////////////////////////////\n\n    var chart =  vis.append('g').attr('class', 'chart1')\n      .attr('transform','translate(' + geom.chart_left + ',' + geom.chart_top + ')');\n    var overlay = chart.append('g').attr('class', 'element').attr('class', 'overlay');\n    var zoom = d3.zoom().scaleExtent([1/3,3]);\n    var zoomNode = overlay.append('rect').attr('class', 'overlay')\n      .attr('x', geom.inner_left).attr('y', geom.inner_top)\n      .attr('width', geom.inner_rawWidth).attr('height', geom.inner_rawHeight)\n      .style('cursor', 'move').call(zoom)\n      .node();\n    zoomNode.__zoom = d3.zoomIdentity;\n    chart.append('rect').attr('class', 'background').attr('width', geom.chart_right-geom.chart_left).attr('height', geom.chart_bottom-geom.chart_top);\n    var interior = chart.append('g').attr('class', 'interior zoomNone')\n      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')')\n      .attr('clip-path', 'url(#clip_viside8cab7c2-7c71-11e7-ac5b-002590fb6404_chart1_inner)');\n    interior.append('rect').attr('class', 'inner').attr('width', geom.inner_width).attr('height', geom.inner_height);\n    var gridGroup = interior.append('g').attr('class', 'grid');\n    var axes = chart.append('g').attr('class', 'axis')\n      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')');\n    vis.append('clipPath').attr('id', 'clip_viside8cab7c2-7c71-11e7-ac5b-002590fb6404_chart1_inner').append('rect')\n      .attr('x', 0).attr('y', 0)\n      .attr('width', geom.inner_rawWidth+1).attr('height', geom.inner_rawHeight+1);\n\n    // Scales //////////////////////////////////////////////////////////////////////////////////////\n\n    var scale_x = d3.scaleLinear().domain([-5, 5.000001])\n      .range([0, geom.inner_width]);\n    var scale_inner = d3.scaleLinear().domain([0,1])\n      .range([-0.5, 0.5]);\n    var scale_y = d3.scaleLinear().domain([-2.5, 2.0000004])\n      .range([geom.inner_height, 0]);\n    var base_scales = [scale_x, scale_y];                           // untransformed original scales\n\n    // Axes ////////////////////////////////////////////////////////////////////////////////////////\n\n    axes.append('g').attr('class', 'x axis')\n      .attr('transform','translate(0,' + geom.inner_rawHeight + ')')\n      .attr('clip-path', 'url(#clip_viside8cab7c2-7c71-11e7-ac5b-002590fb6404_chart1_haxis)');\n    vis.append('clipPath').attr('id', 'clip_viside8cab7c2-7c71-11e7-ac5b-002590fb6404_chart1_haxis').append('polyline')\n      .attr('points', '-1,-1000, -1,-1 -5,5, -1000,5, -100,1000, 10000,1000 10000,-1000');\n    axes.select('g.axis.x').append('text').attr('class', 'title').text('X').style('text-anchor', 'middle')\n      .attr('x',geom.inner_rawWidth/2)\n      .attr('y', geom.inner_bottom - 2.0).attr('dy','-0.27em');\n    axes.append('g').attr('class', 'y axis')\n      .attr('clip-path', 'url(#clip_viside8cab7c2-7c71-11e7-ac5b-002590fb6404_chart1_vaxis)');\n    vis.append('clipPath').attr('id', 'clip_viside8cab7c2-7c71-11e7-ac5b-002590fb6404_chart1_vaxis').append('polyline')\n      .attr('points', '-1000,-10000, 10000,-10000, 10000,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+5) + ', -1000,' + (geom.inner_rawHeight+5) );\n    axes.select('g.axis.y').append('text').attr('class', 'title').text('Y').style('text-anchor', 'middle')\n      .attr('x',-geom.inner_rawHeight/2)\n      .attr('y', 4-geom.inner_left).attr('dy', '0.7em').attr('transform', 'rotate(270)');\n\n    var axis_bottom = d3.axisBottom(scale_x).ticks(Math.min(10, Math.round(geom.inner_width / 33.0)));\n    var axis_left = d3.axisLeft(scale_y).ticks(Math.min(10, Math.round(geom.inner_width / 20)));\n\n    function buildAxes(time) {\n      var axis_x = axes.select('g.axis.x');\n      BrunelD3.transition(axis_x, time).call(axis_bottom.scale(scale_x));\n      var axis_y = axes.select('g.axis.y');\n      BrunelD3.transition(axis_y, time).call(axis_left.scale(scale_y));\n    }\n    zoom.on('zoom', function(t, time) {\n        t = t ||BrunelD3.restrictZoom(d3.event.transform, geom, this);\n        scale_x = t.rescaleX(base_scales[0]);\n        scale_y = t.rescaleY(base_scales[1]);\n        zoomNode.__zoom = t;\n        interior.attr('class', 'interior ' + BrunelD3.zoomLabel(t.k));;\n        build(time || -1);\n    });\n\n    // Define element #1 ///////////////////////////////////////////////////////////////////////////\n\n    elements[0] = function() {\n      var original, processed,                           // data sets passed in and then transformed\n        element, data,                                 // brunel element information and brunel data\n        selection, merged;                                      // d3 selection and merged selection\n      var elementGroup = interior.append('g').attr('class', 'element1'),\n        main = elementGroup.append('g').attr('class', 'main'),\n        labels = BrunelD3.undoTransform(elementGroup.append('g').attr('class', 'labels').attr('aria-hidden', 'true'), elementGroup);\n\n      function makeData() {\n        original = datasets[0];\n        if (filterRows) original = original.retainRows(filterRows);\n        processed = pre(original, 0);\n        processed = post(processed, 0);\n        var f0 = processed.field('x'),\n          f1 = processed.field('y'),\n          f2 = processed.field('#row'),\n          f3 = processed.field('#selection');\n        var keyFunc = function(d) { return f2.value(d) };\n        data = {\n          x:            function(d) { return f0.value(d.row) },\n          y:            function(d) { return f1.value(d.row) },\n          $row:         function(d) { return f2.value(d.row) },\n          $selection:   function(d) { return f3.value(d.row) },\n          x_f:          function(d) { return f0.valueFormatted(d.row) },\n          y_f:          function(d) { return f1.valueFormatted(d.row) },\n          $row_f:       function(d) { return f2.valueFormatted(d.row) },\n          $selection_f: function(d) { return f3.valueFormatted(d.row) },\n          _split:       function(d) { return 'ALL' },\n          _key:         keyFunc,\n          _rows:        BrunelD3.makeRowsWithKeys(keyFunc, processed.rowCount())\n        };\n      }\n\n      // Build element from data ///////////////////////////////////////////////////////////////////\n\n      function build(transitionMillis) {\n        element = elements[0];\n        var w = Math.abs( scale_x(scale_x.domain()[0] + 0.44721359549995765) - scale_x.range()[0] );\n        var x = function(d) { return scale_x(data.x(d))};\n        var h = geom.default_point_size;\n        var y = function(d) { return scale_y(data.y(d))};\n\n        // Define selection entry operations\n        function initialState(selection) {\n          selection\n            .attr('class', 'element point filled')\n            .style('pointer-events', 'none')\n        }\n\n        // Define selection update operations on merged data\n        function updateState(selection) {\n          selection\n            .attr('cx',function(d) { return scale_x(data.x(d))})\n            .attr('cy',function(d) { return scale_y(data.y(d))})\n            .attr('r',Math.min(Math.abs( scale_x(scale_x.domain()[0] + 0.44721359549995765) - scale_x.range()[0] ), geom.default_point_size) / 2);\n        }\n\n        // Define labeling for the selection\n        function label(selection, transitionMillis) {\n        }\n        // Create selections, set the initial state and transition updates\n        selection = main.selectAll('.element').data(data._rows, function(d) { return d.key });\n        var added = selection.enter().append('circle');\n        merged = selection.merge(added);\n        initialState(added);\n        selection.filter(BrunelD3.hasData)\n          .classed('selected', BrunelD3.isSelected(data))\n          .filter(BrunelD3.isSelected(data)).raise();\n        updateState(BrunelD3.transition(merged, transitionMillis));\n\n        BrunelD3.transition(selection.exit(), transitionMillis/3)\n          .style('opacity', 0.5).each( function() {\n            this.remove(); BrunelD3.removeLabels(this); \n        });\n      }\n\n      return {\n        data:           function() { return processed },\n        original:       function() { return original },\n        internal:       function() { return data },\n        selection:      function() { return merged },\n        makeData:       makeData,\n        build:          build,\n        chart:          function() { return charts[0] },\n        group:          function() { return elementGroup },\n        fields: {\n          x:            ['x'],\n          y:            ['y'],\n          key:          ['#row']\n        }\n      };\n    }();\n\n    function build(time, noData) {\n      var first = elements[0].data() == null;\n      if (first) time = 0;                                           // no transition for first call\n      buildAxes(time);\n      if ((first || time > -1) && !noData) {\n        elements[0].makeData();\n      }\n      elements[0].build(time);\n    }\n\n    // Expose the following components of the chart\n    return {\n      elements : elements,\n      interior : interior,\n      scales: {x:scale_x, y:scale_y},\n      zoom: function(params, time) {\n          if (params) zoom.on('zoom').call(zoomNode, params, time);\n          return d3.zoomTransform(zoomNode);\n      },\n      build : build\n    };\n    }();\n\n  function setData(rowData, i) { datasets[i||0] = BrunelD3.makeData(rowData) }\n  function updateAll(time) { charts.forEach(function(x) {x.build(time || 0)}) }\n  function buildAll() {\n    for (var i=0;i<arguments.length;i++) setData(arguments[i], i);\n    updateAll(transitionTime);\n  }\n\n  return {\n    dataPreProcess:     function(f) { if (f) pre = f; return pre },\n    dataPostProcess:    function(f) { if (f) post = f; return post },\n    data:               function(d,i) { if (d) setData(d,i); return datasets[i||0] },\n    visId:              visId,\n    build:              buildAll,\n    rebuild:            updateAll,\n    charts:             charts\n  }\n}\n\n// Data Tables /////////////////////////////////////////////////////////////////////////////////////\n\nvar table1 = {\n   summarized: false,\n   names: ['x', 'y'], \n   options: ['numeric', 'numeric'], \n   rows: [[4.7330106, 0.2236068], [1.8261222, 1.5652476], [-1.0807662, 0.6708204],\n  [0.2608746, -2.0124612], [-1.5279798, -0.6708204], [-4.2112614, 0.2236068]]\n};\n\n// Call Code to Build the system ///////////////////////////////////////////////////////////////////\n\nvar v  = new BrunelVis('viside8cab7c2-7c71-11e7-ac5b-002590fb6404');\nv.build(table1);\n\n    });\n});", "text/plain": "<IPython.core.display.Javascript object>"}}], "source": "%brunel data('pca_pandas') x(x) y(y) :: width=800, height=500", "execution_count": 34}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "", "execution_count": null}, {"metadata": {}, "cell_type": "markdown", "source": "# <span style=\"color:#fa04d9\">Step 6: 3D exercise.</span>"}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "<span style=\"color:blue\">**We have so far worked in this notebook with basic two dimensional datasets in order to get a grasp of PCA. However, as one might suspect, PCA provides added value when working with higher numbers of dimensions, although it becomes more difficult to represent things visually as the number of dimensions increases. In the exercise suggested below, it is proposed to download a three dimensional dataset and perform a PCA analysis on it to determine whether it may be simplified and how.**</span>"}, {"metadata": {}, "cell_type": "markdown", "source": "This cell below will download a three dimensional dataset from github (deletes any existing local version of the file before downloading)"}, {"metadata": {}, "cell_type": "code", "outputs": [{"output_type": "stream", "name": "stdout", "text": "--2017-08-08 14:43:53--  https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/data/threed_data.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.180.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.180.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 559 [text/plain]\nSaving to: \u2018threed_data.csv\u2019\n\n100%[======================================>] 559         --.-K/s   in 0s      \n\n2017-08-08 14:43:54 (129 MB/s) - \u2018threed_data.csv\u2019 saved [559/559]\n\n"}, {"metadata": {}, "output_type": "execute_result", "execution_count": 35, "data": {"text/plain": "[Row(Name='P1', X=0.0, Y=0.0, Z=2.0),\n Row(Name='P2', X=0.0, Y=-2.0, Z=0.0),\n Row(Name='P3', X=2.0, Y=0.0, Z=0.0),\n Row(Name='P4', X=-0.47, Y=-0.63, Z=1.84),\n Row(Name='P5', X=0.63, Y=0.47, Z=1.84)]"}}], "source": "#Delete the file if it exists, download a new copy from GitHub and load it into a dataframe\n!rm threed_data.csv* -f\n!wget https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/data/threed_data.csv\n\nthreed_data = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .option('inferSchema', 'true')\\\n  .load('./threed_data.csv')\n\n# Take a look at a few elements of the dataset.\nthreed_data.take(5)", "execution_count": 35}, {"metadata": {}, "cell_type": "markdown", "source": "### We have a dataset with 4 columns. The first column is the datapoint name and the remaining three columns are the 3D coordinates of the datapoint X, Y and Z."}, {"metadata": {}, "cell_type": "markdown", "source": "### We are now going to define a vector assembler which will take the relevant columns of data and 'concatenate them' into a feature vector conveniently named 'features' (since that's what PCA --and most Spark Machine Learning algorithms-- expects)"}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "# Import VectorAssembler, we haven't used it yet.\nfrom pyspark.ml.feature import VectorAssembler\nfeatureCols = ['X', 'Y', 'Z']\nassembler_definition = VectorAssembler(inputCols=featureCols , outputCol=\"features\")", "execution_count": 36}, {"metadata": {}, "cell_type": "markdown", "source": "### We will also define a Standard Scaler which will transform the input into a zero-mean dataset. This is also a standard transformation when operating several machine learning algorithms including with Spark."}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "from pyspark.ml.feature import StandardScaler\nscaler_definition = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n                        withStd=False, withMean=True)", "execution_count": 37}, {"metadata": {}, "cell_type": "markdown", "source": "Define the pipeline composed of the vector assembler and the scaler"}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "from pyspark.ml import Pipeline\n\npipeline_data_prep_definition = Pipeline(stages=[assembler_definition, scaler_definition])", "execution_count": 38}, {"metadata": {}, "cell_type": "markdown", "source": "Get an instance of the pipeline, which will produce a transformed dataset ready to be fed into the PCA model"}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "pipeline_data_prep_instance = pipeline_data_prep_definition.fit(threed_data)", "execution_count": 39}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "prepped_data_for_pca = pipeline_data_prep_instance.transform(threed_data)", "execution_count": 40}, {"metadata": {}, "cell_type": "markdown", "source": "### We see that the relevant columns were 'concatenated' into a features column (by the assembler), and the features vector was subsequently scaled to a zero-mean format. If we cannot see the last vector due to restrictions of the \"show\" method, we can use something different such as take(n)"}, {"metadata": {}, "cell_type": "code", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----+-----+-----+-----+------------------+--------------------+\n|Name|    X|    Y|    Z|          features|      scaledFeatures|\n+----+-----+-----+-----+------------------+--------------------+\n|  P1|  0.0|  0.0|  2.0|     [0.0,0.0,2.0]|[-0.6996428571428...|\n|  P2|  0.0| -2.0|  0.0|    [0.0,-2.0,0.0]|[-0.6996428571428...|\n|  P3|  2.0|  0.0|  0.0|     [2.0,0.0,0.0]|[1.30035714285714...|\n|  P4|-0.47|-0.63| 1.84|[-0.47,-0.63,1.84]|[-1.1696428571428...|\n|  P5| 0.63| 0.47| 1.84|  [0.63,0.47,1.84]|[-0.0696428571428...|\n|  P6|-0.63|-1.05| 1.58|[-0.63,-1.05,1.58]|[-1.3296428571428...|\n|  P7|-0.67|-1.39| 1.28|[-0.67,-1.39,1.28]|[-1.3696428571428...|\n|  P8| 1.44| 0.66| 1.22|  [1.44,0.66,1.22]|[0.74035714285714...|\n|  P9|-0.62|-1.61| 1.01|[-0.62,-1.61,1.01]|[-1.3196428571428...|\n| P10| 1.62| 0.62|  1.0|   [1.62,0.62,1.0]|[0.92035714285714...|\n| P11|-0.53|-1.77| 0.75|[-0.53,-1.77,0.75]|[-1.2296428571428...|\n| P12|  1.8| 0.51| 0.72|   [1.8,0.51,0.72]|[1.10035714285714...|\n| P13|-0.39| -1.9| 0.49| [-0.39,-1.9,0.49]|[-1.0896428571428...|\n| P14|  1.9| 0.39| 0.49|   [1.9,0.39,0.49]|[1.20035714285714...|\n| P15|-0.22|-1.97| 0.25|[-0.22,-1.97,0.25]|[-0.9196428571428...|\n| P16| 1.97| 0.23| 0.26|  [1.97,0.23,0.26]|[1.27035714285714...|\n| P17| 0.19|-1.98|-0.18|[0.19,-1.98,-0.18]|[-0.5096428571428...|\n| P18| 1.97|-0.24|-0.21|[1.97,-0.24,-0.21]|[1.27035714285714...|\n| P19| 0.47|-1.91|-0.38|[0.47,-1.91,-0.38]|[-0.2296428571428...|\n| P20| 1.87|-0.57|-0.44|[1.87,-0.57,-0.44]|[1.17035714285714...|\n+----+-----+-----+-----+------------------+--------------------+\nonly showing top 20 rows\n\n"}], "source": "prepped_data_for_pca.show()", "execution_count": 41}, {"metadata": {}, "cell_type": "markdown", "source": "### We see that the relevant columns were 'concatenated' into a features column (by the assembler), and the features vector was subsequently scaled to a zero-mean format. If we cannot see the last vector due to restrictions of the \"show\" method, we can use something different such as take(n)"}, {"metadata": {}, "cell_type": "code", "outputs": [{"metadata": {}, "output_type": "execute_result", "execution_count": 42, "data": {"text/plain": "[Row(Name='P1', X=0.0, Y=0.0, Z=2.0, features=DenseVector([0.0, 0.0, 2.0]), scaledFeatures=DenseVector([-0.6996, 0.6525, 1.3525])),\n Row(Name='P2', X=0.0, Y=-2.0, Z=0.0, features=DenseVector([0.0, -2.0, 0.0]), scaledFeatures=DenseVector([-0.6996, -1.3475, -0.6475])),\n Row(Name='P3', X=2.0, Y=0.0, Z=0.0, features=DenseVector([2.0, 0.0, 0.0]), scaledFeatures=DenseVector([1.3004, 0.6525, -0.6475])),\n Row(Name='P4', X=-0.47, Y=-0.63, Z=1.84, features=DenseVector([-0.47, -0.63, 1.84]), scaledFeatures=DenseVector([-1.1696, 0.0225, 1.1925])),\n Row(Name='P5', X=0.63, Y=0.47, Z=1.84, features=DenseVector([0.63, 0.47, 1.84]), scaledFeatures=DenseVector([-0.0696, 1.1225, 1.1925])),\n Row(Name='P6', X=-0.63, Y=-1.05, Z=1.58, features=DenseVector([-0.63, -1.05, 1.58]), scaledFeatures=DenseVector([-1.3296, -0.3975, 0.9325]))]"}}], "source": "prepped_data_for_pca.take(6)", "execution_count": 42}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "from pyspark.ml.feature import PCA\nfrom pyspark.ml.linalg import Vectors\npca = PCA(k=3, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")", "execution_count": 43}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "pca_model_definition = pca.fit(prepped_data_for_pca)", "execution_count": 44}, {"metadata": {}, "cell_type": "code", "outputs": [{"metadata": {}, "output_type": "execute_result", "execution_count": 45, "data": {"text/plain": "DenseMatrix(3, 3, [-0.59, -0.7835, -0.1949, 0.5646, -0.2279, -0.7933, 0.5771, -0.5781, 0.5768], 0)"}}], "source": "pca_model_definition.pc", "execution_count": 45}, {"metadata": {}, "cell_type": "code", "outputs": [{"metadata": {}, "output_type": "execute_result", "execution_count": 46, "data": {"text/plain": "DenseVector([0.5415, 0.4585, 0.0])"}}], "source": "pca_model_definition.explainedVariance", "execution_count": 46}, {"metadata": {}, "cell_type": "markdown", "source": "### From the Eigen values above, we can notice that this dataset is NOT three dimensional but effectively a two dimensional dataset, since the third Eigen value is 0, meaning that the third dimension does not capture any of the variance.\n### The other interesting aspect which can be noticed is that the first two dimensions capture almost the same amount of variance. We can infer from this fact that this 2D dataset is almost equally 'stretching' into these two directions. "}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "pca_output = pca_model_definition.transform(prepped_data_for_pca)", "execution_count": 47}, {"metadata": {}, "cell_type": "markdown", "source": "### Notice how the value of the third dimension is systematically 0"}, {"metadata": {}, "cell_type": "code", "outputs": [{"metadata": {}, "output_type": "execute_result", "execution_count": 48, "data": {"text/plain": "[Row(pcaFeatures=DenseVector([-0.362, -1.6166, -0.0009])),\n Row(pcaFeatures=DenseVector([1.5948, 0.4257, 0.0018])),\n Row(pcaFeatures=DenseVector([-1.1523, 1.0992, -0.0003])),\n Row(pcaFeatures=DenseVector([0.4401, -1.6115, -0.0002])),\n Row(pcaFeatures=DenseVector([-1.0708, -1.2411, -0.0013])),\n Row(pcaFeatures=DenseVector([0.9142, -1.3999, 0.0003])),\n Row(pcaFeatures=DenseVector([1.2627, -1.107, 0.0008])),\n Row(pcaFeatures=DenseVector([-1.5768, -0.3352, -0.0013])),\n Row(pcaFeatures=DenseVector([1.4582, -0.8145, 0.0011])),\n Row(pcaFeatures=DenseVector([-1.6087, -0.0499, -0.0012])),\n Row(pcaFeatures=DenseVector([1.5811, -0.5209, -0.0045])),\n Row(pcaFeatures=DenseVector([-1.5742, 0.2989, 0.0048])),\n Row(pcaFeatures=DenseVector([1.651, -0.206, 0.0015])),\n Row(pcaFeatures=DenseVector([-1.4944, 0.5651, -0.0008])),\n Row(pcaFeatures=DenseVector([1.6524, 0.0963, 0.0017])),\n Row(pcaFeatures=DenseVector([-1.3655, 0.8236, -0.0006])),\n Row(pcaFeatures=DenseVector([1.5021, 0.6712, -0.004])),\n Row(pcaFeatures=DenseVector([-0.9056, 1.3035, 0.0001])),\n Row(pcaFeatures=DenseVector([1.321, 0.972, 0.0018])),\n Row(pcaFeatures=DenseVector([-0.5432, 1.5047, 0.0005])),\n Row(pcaFeatures=DenseVector([0.9712, 1.2973, 0.0017])),\n Row(pcaFeatures=DenseVector([0.2489, 1.5879, 0.0012])),\n Row(pcaFeatures=DenseVector([-1.3165, -0.9665, -0.0014])),\n Row(pcaFeatures=DenseVector([-1.4819, -0.6611, -0.0014])),\n Row(pcaFeatures=DenseVector([-0.6505, -1.5165, 0.0047])),\n Row(pcaFeatures=DenseVector([-0.054, -1.6669, -0.0006])),\n Row(pcaFeatures=DenseVector([0.6708, 1.4677, -0.0043])),\n Row(pcaFeatures=DenseVector([-0.112, 1.6007, 0.0009]))]"}}], "source": "pca_output.select([\"pcaFeatures\"]).collect()", "execution_count": 48}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "my_pandas_data = pca_output.select([\"pcaFeatures\"]).rdd.map(lambda row: (float(row[0][0]), float(row[0][1]))).toDF([\"x\", \"y\"]).toPandas()", "execution_count": 49}, {"metadata": {}, "cell_type": "code", "outputs": [{"metadata": {}, "output_type": "execute_result", "execution_count": 50, "data": {"text/html": "<div>\n<style>\n    .dataframe thead tr:only-child th {\n        text-align: right;\n    }\n\n    .dataframe thead th {\n        text-align: left;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.362023</td>\n      <td>-1.616617</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.594778</td>\n      <td>0.425666</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.152292</td>\n      <td>1.099165</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.440083</td>\n      <td>-1.611507</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.070806</td>\n      <td>-1.241082</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.914232</td>\n      <td>-1.399890</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1.262694</td>\n      <td>-1.107017</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-1.576761</td>\n      <td>-0.335207</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1.458185</td>\n      <td>-0.814472</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-1.608749</td>\n      <td>-0.049941</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1.581117</td>\n      <td>-0.520946</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>-1.574199</td>\n      <td>0.298872</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>1.651041</td>\n      <td>-0.206026</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>-1.494355</td>\n      <td>0.565131</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1.652356</td>\n      <td>0.096296</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>-1.365470</td>\n      <td>0.823566</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>1.502084</td>\n      <td>0.671176</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>-0.905622</td>\n      <td>1.303503</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1.321009</td>\n      <td>0.971972</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>-0.543236</td>\n      <td>1.504690</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.971226</td>\n      <td>1.297322</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.248897</td>\n      <td>1.587907</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>-1.316539</td>\n      <td>-0.966463</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>-1.481929</td>\n      <td>-0.661101</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>-0.650533</td>\n      <td>-1.516463</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>-0.054013</td>\n      <td>-1.666912</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.670809</td>\n      <td>1.467657</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>-0.111985</td>\n      <td>1.600723</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "           x         y\n0  -0.362023 -1.616617\n1   1.594778  0.425666\n2  -1.152292  1.099165\n3   0.440083 -1.611507\n4  -1.070806 -1.241082\n5   0.914232 -1.399890\n6   1.262694 -1.107017\n7  -1.576761 -0.335207\n8   1.458185 -0.814472\n9  -1.608749 -0.049941\n10  1.581117 -0.520946\n11 -1.574199  0.298872\n12  1.651041 -0.206026\n13 -1.494355  0.565131\n14  1.652356  0.096296\n15 -1.365470  0.823566\n16  1.502084  0.671176\n17 -0.905622  1.303503\n18  1.321009  0.971972\n19 -0.543236  1.504690\n20  0.971226  1.297322\n21  0.248897  1.587907\n22 -1.316539 -0.966463\n23 -1.481929 -0.661101\n24 -0.650533 -1.516463\n25 -0.054013 -1.666912\n26  0.670809  1.467657\n27 -0.111985  1.600723"}}], "source": "my_pandas_data", "execution_count": 50}, {"metadata": {}, "cell_type": "code", "outputs": [{"metadata": {}, "output_type": "display_data", "data": {"text/html": "<!--\n  ~ Copyright (c) 2015 IBM Corporation and others.\n  ~\n  ~ Licensed under the Apache License, Version 2.0 (the \"License\");\n  ~ You may not use this file except in compliance with the License.\n  ~ You may obtain a copy of the License at\n  ~\n  ~     http://www.apache.org/licenses/LICENSE-2.0\n  ~\n  ~ Unless required by applicable law or agreed to in writing, software\n  ~ distributed under the License is distributed on an \"AS IS\" BASIS,\n  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  ~ See the License for the specific language governing permissions and\n  ~ limitations under the License.\n  -->\n\n\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/data/jupyter2/e1bf8885-4ff9-49a7-a892-aaea0c0192b7/nbextensions/brunel_ext/brunel.2.3.css\">\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/data/jupyter2/e1bf8885-4ff9-49a7-a892-aaea0c0192b7/nbextensions/brunel_ext/sumoselect.css\">\n\n<style>\n    \n</style>\n\n<div id=\"controlsideb2f8128-7c71-11e7-ac5b-002590fb6404\" class=\"brunel\"/>\n<svg id=\"visideb2f7fa2-7c71-11e7-ac5b-002590fb6404\" width=\"500\" height=\"400\"></svg>", "text/plain": "<IPython.core.display.HTML object>"}}, {"metadata": {}, "output_type": "execute_result", "execution_count": 51, "data": {"application/javascript": "/*\n * Copyright (c) 2015 IBM Corporation and others.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * You may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nrequire.config({\n    waitSeconds: 60,\n    paths: {\n        'd3': '//cdnjs.cloudflare.com/ajax/libs/d3/4.2.1/d3.min',\n        'topojson': '//cdnjs.cloudflare.com/ajax/libs/topojson/1.6.20/topojson.min',\n        'brunel' : '/data/jupyter2/e1bf8885-4ff9-49a7-a892-aaea0c0192b7/nbextensions/brunel_ext/brunel.2.3.min',\n        'brunelControls' : '/data/jupyter2/e1bf8885-4ff9-49a7-a892-aaea0c0192b7/nbextensions/brunel_ext/brunel.controls.2.3.min'\n    },\n    shim: {\n       'brunel' : {\n            exports: 'BrunelD3',\n            deps: ['d3', 'topojson'],\n            init: function() {\n               return {\n                 BrunelD3 : BrunelD3,\n                 BrunelData : BrunelData\n              }\n            }\n        },\n       'brunelControls' : {\n            exports: 'BrunelEventHandlers',\n            init: function() {\n               return {\n                 BrunelEventHandlers: BrunelEventHandlers,\n                 BrunelJQueryControlFactory: BrunelJQueryControlFactory\n              }\n            }\n        }\n\n    }\n\n});\n\nrequire([\"d3\"], function(d3) {\n    require([\"brunel\", \"brunelControls\"], function(brunel, brunelControls) {\n        function  BrunelVis(visId) {\n  \"use strict\";                                                                       // strict mode\n  var datasets = [],                                      // array of datasets for the original data\n      pre = function(d, i) { return d },                         // default pre-process does nothing\n      post = function(d, i) { return d },                       // default post-process does nothing\n      transitionTime = 200,                                        // transition time for animations\n      charts = [],                                                       // the charts in the system\n      vis = d3.select('#' + visId).attr('class', 'brunel');                     // the SVG container\n\n  BrunelD3.addDefinitions(vis);                                   // ensure standard symbols present\n\n  // Define chart #1 in the visualization //////////////////////////////////////////////////////////\n\n  charts[0] = function(parentNode, filterRows) {\n    var geom = BrunelD3.geometry(parentNode || vis.node(), 0, 0, 1, 1, 5, 43, 37, 13),\n      elements = [];                                              // array of elements in this chart\n\n    // Define groups for the chart parts ///////////////////////////////////////////////////////////\n\n    var chart =  vis.append('g').attr('class', 'chart1')\n      .attr('transform','translate(' + geom.chart_left + ',' + geom.chart_top + ')');\n    var overlay = chart.append('g').attr('class', 'element').attr('class', 'overlay');\n    var zoom = d3.zoom().scaleExtent([1/3,3]);\n    var zoomNode = overlay.append('rect').attr('class', 'overlay')\n      .attr('x', geom.inner_left).attr('y', geom.inner_top)\n      .attr('width', geom.inner_rawWidth).attr('height', geom.inner_rawHeight)\n      .style('cursor', 'move').call(zoom)\n      .node();\n    zoomNode.__zoom = d3.zoomIdentity;\n    chart.append('rect').attr('class', 'background').attr('width', geom.chart_right-geom.chart_left).attr('height', geom.chart_bottom-geom.chart_top);\n    var interior = chart.append('g').attr('class', 'interior zoomNone')\n      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')')\n      .attr('clip-path', 'url(#clip_visideb2f7fa2-7c71-11e7-ac5b-002590fb6404_chart1_inner)');\n    interior.append('rect').attr('class', 'inner').attr('width', geom.inner_width).attr('height', geom.inner_height);\n    var gridGroup = interior.append('g').attr('class', 'grid');\n    var axes = chart.append('g').attr('class', 'axis')\n      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')');\n    vis.append('clipPath').attr('id', 'clip_visideb2f7fa2-7c71-11e7-ac5b-002590fb6404_chart1_inner').append('rect')\n      .attr('x', 0).attr('y', 0)\n      .attr('width', geom.inner_rawWidth+1).attr('height', geom.inner_rawHeight+1);\n\n    // Scales //////////////////////////////////////////////////////////////////////////////////////\n\n    var scale_x = d3.scaleLinear().domain([-2, 2.0000004])\n      .range([0, geom.inner_width]);\n    var scale_inner = d3.scaleLinear().domain([0,1])\n      .range([-0.5, 0.5]);\n    var scale_y = d3.scaleLinear().domain([-2, 2.0000004])\n      .range([geom.inner_height, 0]);\n    var base_scales = [scale_x, scale_y];                           // untransformed original scales\n\n    // Axes ////////////////////////////////////////////////////////////////////////////////////////\n\n    axes.append('g').attr('class', 'x axis')\n      .attr('transform','translate(0,' + geom.inner_rawHeight + ')')\n      .attr('clip-path', 'url(#clip_visideb2f7fa2-7c71-11e7-ac5b-002590fb6404_chart1_haxis)');\n    vis.append('clipPath').attr('id', 'clip_visideb2f7fa2-7c71-11e7-ac5b-002590fb6404_chart1_haxis').append('polyline')\n      .attr('points', '-1,-1000, -1,-1 -5,5, -1000,5, -100,1000, 10000,1000 10000,-1000');\n    axes.select('g.axis.x').append('text').attr('class', 'title').text('X').style('text-anchor', 'middle')\n      .attr('x',geom.inner_rawWidth/2)\n      .attr('y', geom.inner_bottom - 2.0).attr('dy','-0.27em');\n    axes.append('g').attr('class', 'y axis')\n      .attr('clip-path', 'url(#clip_visideb2f7fa2-7c71-11e7-ac5b-002590fb6404_chart1_vaxis)');\n    vis.append('clipPath').attr('id', 'clip_visideb2f7fa2-7c71-11e7-ac5b-002590fb6404_chart1_vaxis').append('polyline')\n      .attr('points', '-1000,-10000, 10000,-10000, 10000,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+5) + ', -1000,' + (geom.inner_rawHeight+5) );\n    axes.select('g.axis.y').append('text').attr('class', 'title').text('Y').style('text-anchor', 'middle')\n      .attr('x',-geom.inner_rawHeight/2)\n      .attr('y', 4-geom.inner_left).attr('dy', '0.7em').attr('transform', 'rotate(270)');\n\n    var axis_bottom = d3.axisBottom(scale_x).ticks(Math.min(10, Math.round(geom.inner_width / 33.0)));\n    var axis_left = d3.axisLeft(scale_y).ticks(Math.min(10, Math.round(geom.inner_width / 20)));\n\n    function buildAxes(time) {\n      var axis_x = axes.select('g.axis.x');\n      BrunelD3.transition(axis_x, time).call(axis_bottom.scale(scale_x));\n      var axis_y = axes.select('g.axis.y');\n      BrunelD3.transition(axis_y, time).call(axis_left.scale(scale_y));\n    }\n    zoom.on('zoom', function(t, time) {\n        t = t ||BrunelD3.restrictZoom(d3.event.transform, geom, this);\n        scale_x = t.rescaleX(base_scales[0]);\n        scale_y = t.rescaleY(base_scales[1]);\n        zoomNode.__zoom = t;\n        interior.attr('class', 'interior ' + BrunelD3.zoomLabel(t.k));;\n        build(time || -1);\n    });\n\n    // Define element #1 ///////////////////////////////////////////////////////////////////////////\n\n    elements[0] = function() {\n      var original, processed,                           // data sets passed in and then transformed\n        element, data,                                 // brunel element information and brunel data\n        selection, merged;                                      // d3 selection and merged selection\n      var elementGroup = interior.append('g').attr('class', 'element1'),\n        main = elementGroup.append('g').attr('class', 'main'),\n        labels = BrunelD3.undoTransform(elementGroup.append('g').attr('class', 'labels').attr('aria-hidden', 'true'), elementGroup);\n\n      function makeData() {\n        original = datasets[0];\n        if (filterRows) original = original.retainRows(filterRows);\n        processed = pre(original, 0);\n        processed = post(processed, 0);\n        var f0 = processed.field('x'),\n          f1 = processed.field('y'),\n          f2 = processed.field('#selection'),\n          f3 = processed.field('#row');\n        var keyFunc = function(d) { return f3.value(d) };\n        data = {\n          x:            function(d) { return f0.value(d.row) },\n          y:            function(d) { return f1.value(d.row) },\n          $selection:   function(d) { return f2.value(d.row) },\n          $row:         function(d) { return f3.value(d.row) },\n          x_f:          function(d) { return f0.valueFormatted(d.row) },\n          y_f:          function(d) { return f1.valueFormatted(d.row) },\n          $selection_f: function(d) { return f2.valueFormatted(d.row) },\n          $row_f:       function(d) { return f3.valueFormatted(d.row) },\n          _split:       function(d) { return f2.value(d.row) },\n          _key:         keyFunc,\n          _rows:        BrunelD3.makeRowsWithKeys(keyFunc, processed.rowCount())\n        };\n      }\n      // Aesthetic Functions\n      var scale_color = d3.scaleOrdinal()\n        .domain(['\u2717', '\u2713'])\n        .range([ '#00538A', '#C10020', '#F4C800', '#007D34', '#803E75', '#FF6800', \n          '#817066', '#FFB300', '#F6768E', '#93AA00', '#53377A', '#FF8E00', '#B32851', \n          '#CEA262', '#FF7A5C', '#7F180D', '#593315', '#F13A13', '#232C16']);\n      var color = function(d) { return scale_color(data.$selection(d)) };\n\n      // Build element from data ///////////////////////////////////////////////////////////////////\n\n      function build(transitionMillis) {\n        element = elements[0];\n        var w = geom.default_point_size;\n        var x = function(d) { return scale_x(data.x(d))};\n        var h = geom.default_point_size;\n        var y = function(d) { return scale_y(data.y(d))};\n\n        // Define selection entry operations\n        function initialState(selection) {\n          selection\n            .attr('class', 'element point filled')\n            .style('pointer-events', 'none')\n        }\n\n        // Define selection update operations on merged data\n        function updateState(selection) {\n          selection\n            .attr('cx',function(d) { return scale_x(data.x(d))})\n            .attr('cy',function(d) { return scale_y(data.y(d))})\n            .attr('r',geom.default_point_size / 2)\n            .filter(BrunelD3.hasData)                     // following only performed for data items\n            .style('fill', color);\n        }\n\n        // Define labeling for the selection\n        function label(selection, transitionMillis) {\n        }\n        // Create selections, set the initial state and transition updates\n        selection = main.selectAll('.element').data(data._rows, function(d) { return d.key });\n        var added = selection.enter().append('circle');\n        merged = selection.merge(added);\n        initialState(added);\n        selection.filter(BrunelD3.hasData)\n          .classed('selected', BrunelD3.isSelected(data))\n          .filter(BrunelD3.isSelected(data)).raise();\n        updateState(BrunelD3.transition(merged, transitionMillis));\n\n        BrunelD3.transition(selection.exit(), transitionMillis/3)\n          .style('opacity', 0.5).each( function() {\n            this.remove(); BrunelD3.removeLabels(this); \n        });\n      }\n\n      return {\n        data:           function() { return processed },\n        original:       function() { return original },\n        internal:       function() { return data },\n        selection:      function() { return merged },\n        makeData:       makeData,\n        build:          build,\n        chart:          function() { return charts[0] },\n        group:          function() { return elementGroup },\n        fields: {\n          x:            ['x'],\n          y:            ['y'],\n          key:          ['#row'],\n          color:        ['#selection']\n        }\n      };\n    }();\n\n    function build(time, noData) {\n      var first = elements[0].data() == null;\n      if (first) time = 0;                                           // no transition for first call\n      buildAxes(time);\n      if ((first || time > -1) && !noData) {\n        elements[0].makeData();\n      }\n      elements[0].build(time);\n    }\n\n    // Expose the following components of the chart\n    return {\n      elements : elements,\n      interior : interior,\n      scales: {x:scale_x, y:scale_y},\n      zoom: function(params, time) {\n          if (params) zoom.on('zoom').call(zoomNode, params, time);\n          return d3.zoomTransform(zoomNode);\n      },\n      build : build\n    };\n    }();\n\n  function setData(rowData, i) { datasets[i||0] = BrunelD3.makeData(rowData) }\n  function updateAll(time) { charts.forEach(function(x) {x.build(time || 0)}) }\n  function buildAll() {\n    for (var i=0;i<arguments.length;i++) setData(arguments[i], i);\n    updateAll(transitionTime);\n  }\n\n  return {\n    dataPreProcess:     function(f) { if (f) pre = f; return pre },\n    dataPostProcess:    function(f) { if (f) post = f; return post },\n    data:               function(d,i) { if (d) setData(d,i); return datasets[i||0] },\n    visId:              visId,\n    build:              buildAll,\n    rebuild:            updateAll,\n    charts:             charts\n  }\n}\n\n// Data Tables /////////////////////////////////////////////////////////////////////////////////////\n\nvar table1 = {\n   summarized: false,\n   names: ['x', 'y'], \n   options: ['numeric', 'numeric'], \n   rows: [[-0.3620231, -1.6166172], [1.5947783, 0.4256662], [-1.1522919, 1.0991653],\n  [0.4400825, -1.6115068], [-1.0708061, -1.2410823], [0.9142323, -1.3998899],\n  [1.2626939, -1.1070174], [-1.5767611, -0.3352073], [1.4581854, -0.8144717],\n  [-1.6087493, -0.0499412], [1.5811165, -0.5209459], [-1.5741985, 0.298872], [1.6510406, -0.2060256],\n  [-1.4943548, 0.565131], [1.6523558, 0.0962958], [-1.3654701, 0.823566], [1.5020839, 0.6711756],\n  [-0.9056217, 1.3035026], [1.3210091, 0.9719723], [-0.5432361, 1.5046902], [0.9712265, 1.2973216],\n  [0.2488968, 1.5879066], [-1.3165385, -0.9664633], [-1.4819288, -0.6611012],\n  [-0.6505333, -1.5164634], [-0.0540127, -1.6669118], [0.6708092, 1.4676568], [-0.1119849, 1.600723]]\n};\n\n// Call Code to Build the system ///////////////////////////////////////////////////////////////////\n\nvar v  = new BrunelVis('visideb2f7fa2-7c71-11e7-ac5b-002590fb6404');\nv.build(table1);\n\n    });\n});", "text/plain": "<IPython.core.display.Javascript object>"}}], "source": "import brunel\n# Plot the 5 datapoints\n%brunel data('my_pandas_data') point x(x) y(y) color(#selection)", "execution_count": 51}, {"metadata": {}, "cell_type": "markdown", "source": "### The original 3D dataset which was provided"}, {"metadata": {}, "cell_type": "markdown", "source": "![threed_circle](https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/pictures/circle1.jpg)\n![threed_circle](https://raw.githubusercontent.com/DScienceAtScale/SparkPCA/master/pictures/circle2.jpg)"}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "# Remove the local file (not needed anymore)\n!rm threed_data.csv* -f", "execution_count": 52}, {"metadata": {"collapsed": true}, "cell_type": "code", "outputs": [], "source": "", "execution_count": null}], "nbformat_minor": 1, "nbformat": 4}